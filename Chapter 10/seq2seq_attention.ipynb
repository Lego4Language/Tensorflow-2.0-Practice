{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**遗留问题：**<br>\n",
    "1. dataset.take()函数是抛出式的提取数据还是查询式提取数据？<br>\n",
    "2. 计算损失loss时的mask机制是什么，既然mask掉了一部分值，为什么在计算平均损失的时候仍然使用batch中的样本总数而不是总数减去mask掉的那些样本数？<br>\n",
    "3. 模型训练时不是像预测时那样用前一步的输出作为下一步的输入，而是在decoder端直接采用目标序列的相应词作为输入，这样的不同有什么影响吗？<br>\n",
    "4. 保存子类API实现的模型时tensorflow存在bug，model.save只能将模型保存为.h5py格式的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.3\n",
      "numpy 1.18.1\n",
      "pandas 1.0.1\n",
      "sklearn 0.22.1\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 1. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 加载文件，转换字符集编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之所以要讲unicode编码转换为ascii编码，是因为unicode的词表比较大，运算量相对较大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who lost?\n",
      "¿Quien perdio?\n"
     ]
    }
   ],
   "source": [
    "en_spa_file_path = \"./data_spa_en/spa.txt\"\n",
    "\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) # NFD是标准化的一种操作，将多个ascii码组成的unicode拆开\n",
    "                    if unicodedata.category(c) != \"Mn\") # Mn是重音格式\n",
    "\n",
    "en_sentence = \"Who lost?\"\n",
    "sp_sentence = \"¿Quién perdió?\"\n",
    "\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(sp_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 对文本进行格式化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> who lost ? <end>\n",
      "<start> ¿ quien perdio ? <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    \n",
    "    # 标点符号前后加空格\n",
    "    s = re.sub(r'([?.!,¿])', r\" \\1 \", s)\n",
    "    \n",
    "    # 除了标点符号和字母外都是空格\n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]', \" \", s)\n",
    "    \n",
    "    # 多余的空格变成一个空格\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    \n",
    "    # 去掉前后空格\n",
    "    s = s.rstrip().strip()\n",
    "    \n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> it may be impossible to get a completely error free corpus due to the nature of this kind of collaborative effort . however , if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning , we might be able to minimize errors . <end>\n",
      "<start> puede que sea imposible obtener un corpus completamente libre de errores debido a la naturaleza de este tipo de esfuerzo de colaboracion . sin embargo , si animamos a los miembros a contribuir frases en sus propios idiomas en lugar de experimentar con los idiomas que estan aprendiendo , podriamos ser capaces de minimizar los errores . <end>\n"
     ]
    }
   ],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    sentence_pairs = [line.split(\"\\t\") for line in lines]\n",
    "    preprocessed_sentence_pairs = [\n",
    "        (preprocess_sentence(en), preprocess_sentence(sp)) for en, sp in sentence_pairs\n",
    "    ]\n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "\n",
    "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
    "print(en_dataset[-1])\n",
    "print(sp_dataset[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 把文本序列转换为数字数列（索引）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9402\n",
      "4833\n",
      "16 11\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None, \n",
    "        filters=\"\", \n",
    "        split=\" \")\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, \n",
    "                                                        padding=\"post\") # 在后面添0用作填充\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
    "print(len(input_tokenizer.index_word))\n",
    "print(len(output_tokenizer.index_word))\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "max_length_input = max_length(input_tensor)\n",
    "max_length_output = max_length(output_tensor)\n",
    "print(max_length_input, max_length_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 切分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 6000, 24000, 6000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(\n",
    "    input_tensor, \n",
    "    output_tensor, \n",
    "    test_size=0.2)\n",
    "\n",
    "len(input_train), len(input_eval), len(output_train), len(output_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ---> <start>\n",
      "65 ---> he\n",
      "252 ---> perdido\n",
      "19 ---> mi\n",
      "249 ---> llave\n",
      "3 ---> .\n",
      "2 ---> <end>\n",
      "\n",
      "1 ---> <start>\n",
      "4 ---> i\n",
      "106 ---> lost\n",
      "21 ---> my\n",
      "310 ---> key\n",
      "3 ---> .\n",
      "2 ---> <end>\n"
     ]
    }
   ],
   "source": [
    "# 验证转换是否正确\n",
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print(\"%d ---> %s\" %(t, tokenizer.index_word[t]))\n",
    "            \n",
    "convert(input_train[0], input_tokenizer)\n",
    "print()\n",
    "convert(output_train[0], output_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 生成dataset数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 20\n",
    "steps_per_epoch = len(input_tensor) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor, \n",
    "                 output_tensor,\n",
    "                 batch_size, \n",
    "                 epochs, \n",
    "                 shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (input_tensor, output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(\n",
    "        batch_size, \n",
    "        drop_remainder=True) # 当最后一部分数据不足batch_size时舍弃\n",
    "    return dataset\n",
    "\n",
    "train_dataset = make_dataset(\n",
    "    input_train, output_train, batch_size, epochs, True)\n",
    "\n",
    "eval_dataset = make_dataset(\n",
    "    input_eval, output_eval, batch_size, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 11)\n",
      "tf.Tensor(\n",
      "[[   1    5  506 ...    0    0    0]\n",
      " [   1    5   46 ...    0    0    0]\n",
      " [   1   33   97 ...    0    0    0]\n",
      " ...\n",
      " [   1   42 8722 ...    0    0    0]\n",
      " [   1   59    8 ...    0    0    0]\n",
      " [   1    5  676 ...    0    0    0]], shape=(64, 16), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[   1   22    5   29  363 2726    6    2    0    0    0]\n",
      " [   1   51   40   13 1263   33    6    2    0    0    0]\n",
      " [   1   28  250  534    3    2    0    0    0    0    0]\n",
      " [   1   61   75   63   21  239    3    2    0    0    0]\n",
      " [   1    4   23   12 1036   20    3    2    0    0    0]\n",
      " [   1    7 3806   46    3    2    0    0    0    0    0]\n",
      " [   1   13  560    8   42    3    2    0    0    0    0]\n",
      " [   1    7 1373    5    3    2    0    0    0    0    0]\n",
      " [   1    4   23  290    3    2    0    0    0    0    0]\n",
      " [   1  286   20  194    3    2    0    0    0    0    0]\n",
      " [   1   20    8 2092    3    2    0    0    0    0    0]\n",
      " [   1    9  853  250 3717    3    2    0    0    0    0]\n",
      " [   1   13 4203   57  367    3    2    0    0    0    0]\n",
      " [   1   28  204   43    3    2    0    0    0    0    0]\n",
      " [   1   19  100   12   95    3    2    0    0    0    0]\n",
      " [   1   13  488    8  403    3    2    0    0    0    0]\n",
      " [   1    4   16 2417   21  704    3    2    0    0    0]\n",
      " [   1    4   27   12   45   15  124    3    2    0    0]\n",
      " [   1  180   23   87   18    3    2    0    0    0    0]\n",
      " [   1    4  173   48    3    2    0    0    0    0    0]\n",
      " [   1    5   43   56    4  305    3    2    0    0    0]\n",
      " [   1  752  103 1120    3    2    0    0    0    0    0]\n",
      " [   1   25    5   10  311    6    2    0    0    0    0]\n",
      " [   1    4   16  160   39    3    2    0    0    0    0]\n",
      " [   1   14 1439   65 1243    3    2    0    0    0    0]\n",
      " [   1    4   27   12   64    5    3    2    0    0    0]\n",
      " [   1   20   94   12  596    3    2    0    0    0    0]\n",
      " [   1   31 1288   80   47    3    2    0    0    0    0]\n",
      " [   1  486  212    9    3    2    0    0    0    0    0]\n",
      " [   1   51   22   17   22   19    6    2    0    0    0]\n",
      " [   1   41 2465    3    2    0    0    0    0    0    0]\n",
      " [   1   27   12   93   48   37    2    0    0    0    0]\n",
      " [   1    4  118   99   10   83    3    2    0    0    0]\n",
      " [   1    7  200   46   90    3    2    0    0    0    0]\n",
      " [   1    7    8  102  344    3    2    0    0    0    0]\n",
      " [   1    5   43    4   73    5   37    2    0    0    0]\n",
      " [   1    5   25 2123    3    2    0    0    0    0    0]\n",
      " [   1   21  328  127 2178    3    2    0    0    0    0]\n",
      " [   1   56 1526    6    2    0    0    0    0    0    0]\n",
      " [   1  756   30  350   37    2    0    0    0    0    0]\n",
      " [   1  125   18 3493    3    2    0    0    0    0    0]\n",
      " [   1    4   97    7  120    3    2    0    0    0    0]\n",
      " [   1    7  200 3808    3    2    0    0    0    0    0]\n",
      " [   1    9   11   10 1033   63   98    3    2    0    0]\n",
      " [   1   61   18  140  644    3    2    0    0    0    0]\n",
      " [   1   87 1723    3    2    0    0    0    0    0    0]\n",
      " [   1   17   24   36   13 1441    3    2    0    0    0]\n",
      " [   1   13  203  911  394    3    2    0    0    0    0]\n",
      " [   1   51  167   40    5  158    6    2    0    0    0]\n",
      " [   1    8   20   30   83    6    2    0    0    0    0]\n",
      " [   1    5   23   33    3    2    0    0    0    0    0]\n",
      " [   1   17   24   36  154   54    3    2    0    0    0]\n",
      " [   1 1216 3297    3    2    0    0    0    0    0    0]\n",
      " [   1   79   25    5  458    6    2    0    0    0    0]\n",
      " [   1    7   11   36   60    3    2    0    0    0    0]\n",
      " [   1    7    8  963    3    2    0    0    0    0    0]\n",
      " [   1    4   29  157   36  913    3    2    0    0    0]\n",
      " [   1    4   27   12  271    9   37    2    0    0    0]\n",
      " [   1    4   69 2104  361    3    2    0    0    0    0]\n",
      " [   1    4   23   12  597  150    3    2    0    0    0]\n",
      " [   1    7    8  102  368    3    2    0    0    0    0]\n",
      " [   1   28   24   13  794  571    3    2    0    0    0]\n",
      " [   1   28   27   12   74    3    2    0    0    0    0]\n",
      " [   1   40    5   64   10  311    6    2    0    0    0]], shape=(64, 11), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_units = 256  # 词向量维度（定义两种语言维度相同）\n",
    "units = 1024    # 隐藏层单元数（定义encoder和decoder端隐藏节点数相同）\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1   # 因为有一个用于表示pad的数字0\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_output.shape:  (64, 16, 1024)\n",
      "sample_hidden.shape:  (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units,\n",
    "                 batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, \n",
    "                                                embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state= True,\n",
    "                                    recurrent_initializer=\"glorot_uniform\")\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "    \n",
    "encoder = Encoder(input_vocab_size, embedding_units,\n",
    "                  units, batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "\n",
    "print(\"sample_output.shape: \", sample_output.shape)\n",
    "print(\"sample_hidden.shape: \", sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_results.shape (64, 1024)\n",
      "attention_weights.shape (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, decoder_hidden, encoder_outputs):\n",
    "        # decoder_hidden.shape: (batch_size, units)\n",
    "        # encoder_outputs.shape: (batch_size, length, units)\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
    "\n",
    "        # before V: (batch_size, length, units)\n",
    "        # after V: (batch_size, length, 1)\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n",
    "\n",
    "        #shape: (batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        #context_vector.shape : (batch_size, length, units)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "\n",
    "        #context_viector.shape: (batch_size, units)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "attention_model = BahdanauAttention(units=10)\n",
    "attention_results, attention_weights = attention_model(\n",
    "    sample_hidden, sample_output)\n",
    "\n",
    "print(\"attention_results.shape\", attention_results.shape)\n",
    "print(\"attention_weights.shape\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_output.shape:  (64, 4834)\n",
      "decoder_hidden.shape:  (64, 1024)\n",
      "decoder_attention_weights.shape:  (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units,\n",
    "                 decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = keras.layers.Embedding(\n",
    "            vocab_size, embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer=\"glorot_uniform\")\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "        \n",
    "    def call(self, x, hidden, encoding_outputs):\n",
    "        # context_vector.shape: (batch_size, units)\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            hidden, encoding_outputs)\n",
    "        \n",
    "        # before embedding: x.shape: (batch_size, 1)\n",
    "        # after embedding: x.shape: (batch_size, 1, embeding_units)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        combined_x = tf.concat(\n",
    "            [tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # output.shape: [batch_size, 1, decoding_units]\n",
    "        # state.shape: [batch_size, decoding_units]\n",
    "        output, state = self.gru(combined_x)\n",
    "        \n",
    "        # output.shape: [batch_size, decoding_units]\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output.shape: [batch_size, vocab_size]\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output, state, attention_weights\n",
    "    \n",
    "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
    "\n",
    "outputs = decoder(tf.random.uniform((batch_size, 1)),\n",
    "                  sample_hidden,\n",
    "                  sample_output)\n",
    "decoder_output, decoder_hidden, decoder_aw = outputs\n",
    "\n",
    "print(\"decoder_output.shape: \", decoder_output.shape)\n",
    "print(\"decoder_hidden.shape: \", decoder_hidden.shape)\n",
    "print(\"decoder_attention_weights.shape: \", decoder_aw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction=\"none\")      # 先不进行汇总计算\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, encoding_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(\n",
    "            inp, encoding_hidden)\n",
    "        \n",
    "        decoding_hidden = encoding_hidden\n",
    "        \n",
    "        # eg: <start> I am here <end>\n",
    "        # 1. <start> -> I\n",
    "        # 2. I -> am\n",
    "        # 3. am -> here\n",
    "        # 4. here -> <end>\n",
    "        for t in range(0, targ.shape[1]-1):\n",
    "            decoding_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "            predictions, decoding_hidden, attention_weight = decoder(\n",
    "                decoding_input, decoding_hidden, encoding_outputs)\n",
    "            loss += loss_function(targ[:, t+1], predictions)\n",
    "            \n",
    "    batch_loss = loss / int(targ.shape[0])  # 有些被mask掉了，平均数为何不是除以非零的数量和而是总的数量??????????????????????????\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradient = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradient, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is this a trianing model ? (yes or no)--->  no\n"
     ]
    }
   ],
   "source": [
    "train_model = input(\"Is this a trianing model ? (yes or no)---> \")\n",
    "if train_model == \"yes\":\n",
    "    for epoch in range(1, epochs+1):\n",
    "        start = time.time()\n",
    "\n",
    "        encoding_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(\n",
    "            train_dataset.take(steps_per_epoch), 1):\n",
    "            batch_loss = train_step(inp, targ, encoding_hidden)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print(\"Epoch {} Batch {} Loss {:.4f}\".format(\n",
    "                    epoch, batch, batch_loss.numpy()))\n",
    "\n",
    "        print(\"Epoch {} Loss {:.4f}\".format(epoch,\n",
    "                                            total_loss / steps_per_epoch))\n",
    "        print(\"Time take for 1 epoch {} sec\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 预测及结果检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_sentence):\n",
    "    attention_matrix = np.zeros((max_length_output, max_length_input))\n",
    "    input_sentence = preprocess_sentence(input_sentence)\n",
    "    \n",
    "    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(\" \")]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences(\n",
    "    [inputs],\n",
    "    maxlen=max_length_input,\n",
    "    padding=\"post\")\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    results = \"\"\n",
    "#     encoding_hidden = encoder.initialize_hidden_state()\n",
    "    encoding_hidden = tf.zeros((1, units))\n",
    "    \n",
    "    encoding_outputs, encoding_hidden = encoder(inputs, encoding_hidden)\n",
    "    decoding_hidden = encoding_hidden\n",
    "    \n",
    "    # eg: <start> -> A\n",
    "    # A -> B -> C -> D\n",
    "    \n",
    "    # decoding_input.shape: (1, 1)\n",
    "    decoding_input = tf.expand_dims(\n",
    "        [output_tokenizer.word_index[\"<start>\"]], 0)\n",
    "    for t in range(max_length_output):\n",
    "        predictions, decoding_hidden, attention_weights = decoder(\n",
    "            decoding_input, decoding_hidden, encoding_outputs)\n",
    "        \n",
    "        # attention_weights.shape: (batch_size, input_length, 1) (1, 16, 1)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        # predictions.shape: (batch_size, vocab_size) (1, 4834)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        results += output_tokenizer.index_word[predicted_id] + \" \"\n",
    "        if output_tokenizer.index_word[predicted_id] == \"<end>\":\n",
    "            return results, input_sentence, attention_matrix\n",
    "        \n",
    "        decoding_input = tf.expand_dims([predicted_id], 0)\n",
    "    return results, input_sentence, attention_matrix\n",
    "\n",
    "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention_matrix, cmap=\"viridis\")\n",
    "    \n",
    "    font_dict = {\"fontsize\": 14}\n",
    "    \n",
    "    ax.set_xticklabels([\"\"] + input_sentence, \n",
    "                       fontdict=font_dict, \n",
    "                       rotation=90)\n",
    "    ax.set_yticklabels([\"\"] + predicted_sentence,\n",
    "                       fontdict=font_dict)\n",
    "    plt.show()\n",
    "    \n",
    "def translate(input_sentence):\n",
    "    results, input_sentence, attention_matrix = evaluate(input_sentence)\n",
    "    print(\"Input: %s\" %(input_sentence))\n",
    "    print(\"Predicted translate: %s\" %(results))\n",
    "    \n",
    "    attention_matrix = attention_matrix[:len(results.split(\" \")),\n",
    "                                        :len(input_sentence.split(\" \"))]\n",
    "    plot_attention(attention_matrix, \n",
    "                   input_sentence.split(\" \"),\n",
    "                   results.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> tom es mi nombre , tengo anos . <end>\n",
      "Predicted translate: raised occupied get confirm list computer eventually popular gander computer eventually \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u\"Tom es mi nombre, tengo 29 años.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.保存和提取模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_model == \"yes\":\n",
    "    encoder_path = \"./seq2seq_attention_save_encoder/\"\n",
    "    decoder_path = \"./seq2seq_attention_save_decoder/\"\n",
    "\n",
    "    def create_dir(path):\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "    create_dir(encoder_path)\n",
    "    create_dir(decoder_path)\n",
    "\n",
    "    # 查看参数\n",
    "    def print_param_info(coder):\n",
    "        for matrix in coder.weights:\n",
    "            print(matrix.name, matrix.shape)\n",
    "    print_param_info(encoder)\n",
    "    print(\"-------------\")\n",
    "    print_param_info(decoder)\n",
    "\n",
    "    # 保存参数\n",
    "    encoder.save_weights(encoder_path)\n",
    "    decoder.save_weights(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> esta es mi familia <end>\n",
      "Predicted translate: this is my family . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载参数\n",
    "encoder_path = \"./seq2seq_attention_save_encoder/\"\n",
    "decoder_path = \"./seq2seq_attention_save_decoder/\"\n",
    "encoder.load_weights(encoder_path)\n",
    "decoder.load_weights(decoder_path)\n",
    "translate(u\"esta es mi familia\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
