{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Synset' object has no attribute 'hypernums'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1c4cd0a7f231>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpanda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"panda.n.01\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhyper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypernums\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpanda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\installation\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36mclosure\u001b[1;34m(self, rel, depth)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0msynset_offsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbreadth_first\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msynset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msynset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynset_offsets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\installation\\lib\\site-packages\\nltk\\util.py\u001b[0m in \u001b[0;36mbreadth_first\u001b[1;34m(tree, children, maxdepth)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                 \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1c4cd0a7f231>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# nltk.download(\"wordnet\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpanda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"panda.n.01\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhyper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypernums\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpanda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Synset' object has no attribute 'hypernums'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "# import nltk\n",
    "# nltk.download(\"wordnet\")\n",
    "panda = wn.synset(\"panda.n.01\")\n",
    "hyper = lambda s: s.\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**遗留问题：**<br>\n",
    "1. 缩放点积中的dk是方差，为什么在代码中是用的最后一个维度的大小表示？<br>\n",
    "2. tf.shape()和tensor.shape的区别是什么？<br>\n",
    "3. 训练时创建masks时inp和targ_inp的数据形式好像发生了变化？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.3\n",
      "numpy 1.18.1\n",
      "pandas 1.0.1\n",
      "sklearn 0.22.1\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据准备和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found a different version 1.0.0 of dataset ted_hrlr_translate in data_dir C:\\Users\\NEO\\tensorflow_datasets. Using currently defined version 0.0.1.\n",
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='ted_hrlr_translate',\n",
      "    version=0.0.1,\n",
      "    description='Data sets derived from TED talk transcripts for comparing similar language pairs\n",
      "where one is high resource and the other is low resource.\n",
      "',\n",
      "    urls=['https://github.com/neulab/word-embeddings-for-nmt'],\n",
      "    features=Translation({\n",
      "        'en': Text(shape=(), dtype=tf.string),\n",
      "        'pt': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    total_num_examples=54781,\n",
      "    splits={\n",
      "        'test': 1803,\n",
      "        'train': 51785,\n",
      "        'validation': 1193,\n",
      "    },\n",
      "    supervised_keys=('pt', 'en'),\n",
      "    citation=\"\"\"@inproceedings{Ye2018WordEmbeddings,\n",
      "      author  = {Ye, Qi and Devendra, Sachan and Matthieu, Felix and Sarguna, Padmanabhan and Graham, Neubig},\n",
      "      title   = {When and Why are pre-trained word embeddings useful for Neural Machine Translation},\n",
      "      booktitle = {HLT-NAACL},\n",
      "      year    = {2018},\n",
      "      }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "examples, info = tfds.load(\"ted_hrlr_translate/pt_to_en\",\n",
    "                           with_info=True,\n",
    "                           as_supervised=True)\n",
    "train_examples, val_examples = examples[\"train\"], examples[\"validation\"]\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'os astr\\xc3\\xb3nomos acreditam que cada estrela da gal\\xc3\\xa1xia tem um planeta , e especulam que at\\xc3\\xa9 um quinto deles tem um planeta do tipo da terra que poder\\xc3\\xa1 ter vida , mas ainda n\\xc3\\xa3o vimos nenhum deles .'\n",
      "b\"astronomers now believe that every star in the galaxy has a planet , and they speculate that up to one fifth of them have an earth-like planet that might be able to harbor life , but we have n't seen any of them .\"\n",
      "-------------------\n",
      "b'o problema \\xc3\\xa9 que nunca vivi l\\xc3\\xa1 um \\xc3\\xbanico dia .'\n",
      "b\"except , i 've never lived one day of my life there .\"\n",
      "-------------------\n",
      "b'agora aqui temos imagens sendo extra\\xc3\\xaddas em tempo real diretamente do feed ,'\n",
      "b'now here are live images being pulled straight from the feed .'\n",
      "-------------------\n",
      "b'agora : um , dois , tr\\xc3\\xaas , vai .'\n",
      "b'so : one , two , three , go .'\n",
      "-------------------\n",
      "b'eventualmente , vamos ver se teremos todos os sentidos humanos empregues , e se vamos ter meios para viver a hist\\xc3\\xb3ria qualquer que seja a via escolhida .'\n",
      "b'eventually , we can see if we will have all of our human senses employed , and we will have agency to live the story in any path we choose .'\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "for pt, en in train_examples.take(5):\n",
    "    print(pt.numpy())\n",
    "    print(en.numpy())\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples),\n",
    "    target_vocab_size=2 ** 13)\n",
    "pt_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples),\n",
    "    target_vocab_size=2 ** 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is: [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
      "The original string is: Transformer is awesome.\n",
      "7915 --> \"T\"\n",
      "1248 --> \"ran\"\n",
      "7946 --> \"s\"\n",
      "7194 --> \"former \"\n",
      "13 --> \"is \"\n",
      "2799 --> \"awesome\"\n",
      "7877 --> \".\"\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"Transformer is awesome.\"\n",
    "\n",
    "tokenized_string = en_tokenizer.encode(sample_string)\n",
    "print(\"Tokenized string is: {}\".format(tokenized_string))\n",
    "\n",
    "origin_string = en_tokenizer.decode(tokenized_string)\n",
    "print(\"The original string is: {}\".format(origin_string))\n",
    "\n",
    "assert origin_string == sample_string\n",
    "\n",
    "for token in tokenized_string:\n",
    "    print('{} --> \"{}\"'.format(token, en_tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 38) (64, 40)\n",
      "(64, 39) (64, 35)\n",
      "(64, 39) (64, 39)\n",
      "(64, 39) (64, 39)\n",
      "(64, 39) (64, 36)\n"
     ]
    }
   ],
   "source": [
    "buffer_size = 20000\n",
    "batch_size = 64\n",
    "max_length = 40\n",
    "\n",
    "def encode_to_subword(pt_sentence, en_sentence):\n",
    "    pt_sequence = [pt_tokenizer.vocab_size] \\\n",
    "    + pt_tokenizer.encode(pt_sentence.numpy()) \\\n",
    "    + [pt_tokenizer.vocab_size + 1]\n",
    "    \n",
    "    en_sequence = [en_tokenizer.vocab_size] \\\n",
    "    + en_tokenizer.encode(en_sentence.numpy()) \\\n",
    "    + [en_tokenizer.vocab_size + 1]\n",
    "    \n",
    "    return pt_sequence, en_sequence\n",
    "\n",
    "def filter_by_max_length(pt, en):\n",
    "    return tf.logical_and(tf.size(pt) <= max_length,\n",
    "                          tf.size(en) <= max_length)\n",
    "\n",
    "def tf_encode_to_subword(pt_sentence, en_sentence):\n",
    "    return tf.py_function(encode_to_subword,\n",
    "                          [pt_sentence, en_sentence],\n",
    "                          [tf.int64, tf.int64])\n",
    "\n",
    "train_dataset = train_examples.map(tf_encode_to_subword)\n",
    "train_dataset = train_dataset.filter(filter_by_max_length)\n",
    "train_dataset = train_dataset.shuffle(\n",
    "    buffer_size).padded_batch(\n",
    "    batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "valid_dataset = val_examples.map(tf_encode_to_subword)\n",
    "valid_dataset = valid_dataset.filter(\n",
    "    filter_by_max_length).padded_batch(\n",
    "    batch_size, padded_shapes=([-1], [-1]))\n",
    "\n",
    "for pt_batch, en_batch in valid_dataset.take(5):\n",
    "    print(pt_batch.shape, en_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .', shape=(), dtype=string)\n",
      "tf.Tensor(b'but what if it were active ?', shape=(), dtype=string)\n",
      "tf.Tensor(b\"but they did n't test for curiosity .\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for pt, en in train_examples.take(3):\n",
    "    print(en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 2. 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "# PE(pos, 2i)    =  sin( pos / 10000^( 2i / d_model ))\n",
    "# PE(pos, 2i+1)  =  cos( pos / 10000^( 2i / d_model ))\n",
    "\n",
    "# pos.shape    : [sentence_length, 1]\n",
    "# i.shape      : [1, d_model]\n",
    "# result.shape : [sentence_length, d_model]\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000,\n",
    "                              (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def get_position_embedding(sentence_length, d_model):\n",
    "    angle_rates = get_angles(np.arange(sentence_length)[:, np.newaxis],\n",
    "                             np.arange(d_model)[np.newaxis, :],\n",
    "                             d_model)\n",
    "    # sines.shape   : [sentence_length, d_model / 2]\n",
    "    # cosines.shape : [sentence_length, d_model / 2]\n",
    "    sines = np.sin(angle_rates[ : , 0::2])\n",
    "    cosines = np.cos(angle_rates[ : , 1::2])\n",
    "    \n",
    "    # position_embedding.shape: [sentence_length, d_model]\n",
    "    position_embedding = np.concatenate([sines, cosines], axis=-1)\n",
    "\n",
    "    \n",
    "    position_embedding.shape: [1, sentence_length, d_model]\n",
    "    position_embedding = position_embedding[np.newaxis, ...]\n",
    "\n",
    "#     angle_rates[:, 0::2] = np.sin(angle_rates[:, 0::2])\n",
    "#     angle_rates[:, 1::2] = np.cos(angle_rates[:, 1::2])\n",
    "#     position_embedding = angle_rates[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(position_embedding, dtype=tf.float32)\n",
    "\n",
    "position_embedding = get_position_embedding(50, 512)\n",
    "print(position_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEXCAYAAACqIS9uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydd3wU1fr/3zuzu8kmm94JIfTeQSGiIEhTCCDg7yIoVq4NEK5yryKKYLkWrogo4NWvXkUUEaQpRsSG0gRUIIB0COm9bd+Z/f2xySYbwiaUKNHzfr3Oa3fm7MycSSZPzj7neT6PxuVyuRAIBALBXwbpjx6AQCAQCH5fhOEXCASCvxjC8AsEAsFfDGH4BQKB4C+GMPwCgUDwF0MYfoFAIPiLIQz/X4z09HQ6dOjA6NGjPW3UqFGsXr36os/5xBNPsH37dgDmzJlDamrqOfsvlV27dtG1a1evcY8ePZo777zzgs6zePFi5s+ff0ljOXDgAIMGDaq1b/78+SxevBiAKVOmcPz48Uu6lkDQEGj/6AEIfn/8/f1Zv369ZzsnJ4eRI0fSuXNn2rdvf8Hne+655zzvt2/fzt/+9rdz9l8OmjVr5jXuK5233nrrjx6CQFArwvALiImJITExkdOnT9O+fXveeOMNPv/8c2RZpkWLFjz55JNERUWxefNmli5dikajQZZl/vnPf3LVVVdx++23M2nSJA4fPkxubi6PPvooL730EgsWLGDSpEkMHz6cLVu28Prrr6OqKoGBgTz++ON07dqVxYsXk5GRQV5eHhkZGcTExPDyyy8THR19QfewePFi0tLSyMnJIS8vj06dOtGnTx/WrVtHeno6s2bNYuTIkQCcOHGCSZMmUVJSQocOHZg7dy5Go5GcnBzmz59PVlYWDoeDESNGcP/99wPw4Ycf8t5772E0Gmnbtq3nuuXl5TzxxBP89ttvREdHI8syvXr1AmDQoEEsWrQIs9nMwoULSUhI4NixYzidTubNm0evXr0oLCzk8ccfJy0tjdDQUKKiomjTpg3Tpk27TL9dgeBchKtHwC+//EJaWhrdunVjzZo1/PDDD6xevZqNGzfSpk0bHnvsMQBeeukl5s6dy6effsrDDz/Mrl27vM4zc+ZMoqOjWbBgAd26dfPsP3HiBHPnzmXx4sVs2LCB6dOn8+CDD1JeXg7Anj17WLRoESkpKRgMBlauXFnrONPS0s5x9SxdutTTv3fvXt544w3Wrl3L1q1bOXHiBCtWrODJJ5/0uF8qz7N48WI2btyIy+XynGPWrFmMGzeOTz/9lNWrV7N9+3Y2bdrE4cOHef311/nggw9Ys2YNOp3Oc67XXnsNf39/UlJSWLRoEadOnap17Pv37+fuu+9m3bp1jB07loULFwLw7LPP0rp1a7744gsWLVrEzz//XO/fm0BwsYgZ/18Qq9XK6NGjAVAUhbCwMF5++WXi4uLYunUrY8eOJSAgAIDJkyezbNky7HY7I0aMYOrUqQwYMIB+/foxZcqUel1v586d9O3bl4SEBACSkpIIDw/3rAVcffXVGI1GADp27EhJSUmt56nL1XPNNdcQFBQEQHR0NNddd53nuOLiYs/nhgwZQnh4OADjxo3jpZdewmw2s3v3bkpKSli0aBEAZrOZ3377jezsbPr160dUVBQAf/vb3/jxxx8B2LFjB7Nnz0aj0RAeHs6QIUNqHVuTJk3o0KGD5x7Xrl0LwPfff+95Hx0dzfDhw8//gxQILhPC8P8Fqenjr46qqmg0Gq9tp9MJuGf048aNY9u2bXz66ae888479VoUrnlOAJfL5Tmvv7+/Z79Go+Fi5aP0er3XtlZb++Mty7LX2LRaLaqq4nK5WLlyJQaDAYDCwkL8/Pz4+OOPvcZU/fjKezlfXyXnu0etVut1vCSJL+GChkc8ZQIvrrvuOtasWYPZbAZg+fLlXHXVVUiSxKBBg7BYLNx6663MnTuXI0eOYLfbvY6XZdlj0CtJSkrixx9/5OzZs4B7lpyVleXlDvo9+eabbygpKUFRFFatWkX//v0xGo10796dd999F4DS0lJuvfVWvv76a/r168e2bdvIzs4G8MzQwf3zWr16NaqqUlJSwtdff31BYxkwYIDnn2dRURFbtmw555+kQHC5ETN+gRfjx48nKyuLW265BVVVSUxMZMGCBWi1WmbPns2jjz6KVqtFo9Hw/PPPnzPLHjJkCLNmzeLpp5/27GvdujVz585l6tSpKIqCv78/y5Yt87hl6kulj78m77zzzgWdp1WrVtx3332UlpbSq1cv/v73vwOwYMECnnnmGZKTk7Hb7YwcOZJRo0YBbv//HXfcQWBgIF27dvWca9q0acydO5cbb7yR8PBwr4Xf+vD4448zZ84ckpOTCQ0NpUmTJl7fDgSChkAjZJkFgj+OFStW0LFjR3r06IHdbmfixIlMmzaNAQMG/NFDE/yJETN+geAPpHXr1jzzzDOoqorD4WD48OHC6AsanAad8d9+++0UFhZ6Ftnmz5+PyWTi3//+NzabjRtvvJGZM2c21OUFAoHgiqS8vJwJEyawbNkymjZt6tV3+PBhnnjiCUwmE71792bevHlotVoyMzOZNWsWBQUFtGjRggULFhAYGHhR12+wxV2Xy8Xp06dZv369p7Vr147Zs2ezZMkSNm3aRGpqKt9//31DDUEgEAiuOPbt28ett97K6dOna+2fNWsWTz31FF9++SUul4tVq1YBMG/ePCZOnEhKSgqdO3dmyZIlFz2GBjP8J0+eBODuu+9m1KhRfPDBB+zfv5/ExEQSEhLQarUkJyeTkpLSUEMQCASCK45Vq1Yxd+7cWrPTMzIysFqtdO/eHYCxY8eSkpKCw+Fg9+7dDBs2zGv/xdJgPv7S0lKSkpJ48skncTgcTJ48mXvvvdeTBAPuhJWcnJyGGoJAIBD8LpSWllJaWnrO/uDgYIKDg732+dKwys3N9bKRUVFR5OTkUFRUhNFo9LjNK/dfLA1m+Hv06EGPHj082+PHj+e1117z6JiA2x0kYpYFAsGVSmFJOeEhxjo/p9frGTt27DlZ51OnTr0g3aWayY6VNrI2W3kptrPBDP+ePXtwOBwkJSUB7huIj48nLy/P85m8vLwLFuOy2WxI5fmsSnMRP20KU4I7syY6g/ZTJ9Lpqe/Z/0ALNsxZw7anXuHL5auZ9/R9BN97F7IGmqxfw/+b/ipNuibxxdTOfDtsMjsKLNw2tj3NJo7jE00XFv3fdxSd3EdEm96MTu7B9H7N4Ms32f/fb9l1oog8m4KkgcQAHZ1bhpI4qB0RAwbgapNEmlXmp/QSdp0s4PTZEorzTJjyMnDaTNhNJShWMy5VAUDS6tHIOnT+Ach+AegCgtAFBCHJOvwC/ND769D5yej9dOj9ZQx+WoINWgL8tBj1WgL8ZPRaiQA/LX6SBr1WRi9L+GkldBLoZQmtJKGTNGglCa0MGg3IGg0SIGk0aDQgaaq9x/0wSRXPU+VjpaHqIZM0Vfsrtz1UexClGs/k+R5RqZ4P7/k+9kNaKf1jdTi1/jzfYTAb+41hZegBWr7yX0a8uJUhy+dz208p3DR1KSk9ctm5Yi/H/rOM/y35iGvGjmT2vmUs/+QQs1c/wd92BJP6+Tr6TZrAO9fp+Hrco3yXZ2Z4q1BaDm1P0LRnmfPFEbZ9uZvS9CMYIprQsndvbhvUihFtIpB+/oz0jZspPlnEkcMFnDLbMTtd6CQI08kkBGiJiwsisn0EYe2bYWzVAl3z9mAIxhnchDJVS5FNIafcTna5jexSK1klVvJKbZSabNjMTpxOBbtVwWl34rArOG0WFJsVVXGgOh0Vr3ZcTgcAquLEpSqoFc+dS1FxudzvUVWvn2Xls3m+7cZEfHQY3/1v9iWfJzzEyMB7XiQjp+j814oJ49v/+xfr169HUbx/ZjVn+3URGxvrZSPz8/OJjo4mPDycsrIyFEVBluWLsp3VaTDDX1ZWxmuvvcbKlStxOBysXbuWefPmMWPGDM6cOUPTpk357LPPGDdu3AWd1wWgKpTbXVjSMzkTFoedHLCZOZNVgMscTvnZDPJNdtKyCim3K2jPZiJrwK6opGUXQYIFVAXz2UwK88w4CsLBZqZcUkjPKyM/qwBLWDlFFoc7nd5UgjUri+K0AgqsTmQNhAXqsRqsKMWRYDODS8WhSJTbFfJNdrJKrBQWmCnLK8VpKcdWXoTTUu5l+CWtHp3BjtbfgT4QdIESklaPv1XFz+BEb9Di56/g59AS4FCwqypGxYVDceFwudBrJRTAIUv4qeDUulBdLpySBtUFOglU2f3ehdu4qxXGXtbgMfyV711UvFYa/opXCW/DfT5TXX1/fUPFLjWkzOJQwCXjcrkoOJNOWutiHI5cXC4XmUUWis5koKgu0nKKUYoLMJ3NpNjiIC27iNblNhwFuRSeyQCbmewSPWeyCmhTZgOn+/kqyC7HarCgFEfhcrnIN9k5m1dKcVYBgc4ADCVWTHbF/ZxYTTgL8rBl51N+NpeicjvlThW9pEHSy4QH6rASjCNSQS014rJEgsMGege4VFSX+3drdaqY7AolVieFZge55TaKy2xYTQ6cDgWbxYnDVtGsZhSbBdVpR3HaUZ12VIf7FUB1OmoYfsXzDNZl6Buz4b+cZOaVkpZTfN5+jeSW6YiLi7vka8XHx+Pn58fevXvp1asX69evp3///uh0Onr37s2mTZtITk5m3bp19O/f/6Kv02CGf+DAgezbt48xY8agqioTJ06kR48evPDCC0ybNg2bzcaAAQOEKJVAILiykSSPcT9f/6UyZcoUpk+fTpcuXViwYAFz5syhvLycTp06MXnyZADmzp3LY489xtKlS4mLi+OVV1656Os1aALXjBkzmDFjhte+pKQkNmzY0JCXFQgEgsuGRiP7NPwajY9/Cj745ptvPO+rF+1p3759reKH8fHxLF++/KKuVZNGJ9K284YxXLv0CLenfczQ794j8ZpkVmw+ydcRA2iWNJJvH/2IIfcl8f0Xv+BSFW7rHMm2AjNdxnfi7V1ncJhK6NEjDnXPJg6U2Ijx0xLfvzu06cP3R3IxF2QgafWExETTJT4Ev7Jsio+epTirnHKn2yeqlzSE62UCIg0ExkagjYjFrjVQbHVQZLZTUG7HbvH+Gq467Od8ddbIMhpJQtLp0Ujuh0vWatFIGjRShe9dAo2kQa+VkCUJWaNBlqo1jQZJ0iBXuGwkjQZJo6n4HJ5Xtw/f7Yyp7i+vy8Ne/QGp6bevvq25CP/+5aDZk3fQ+fHv2XlVf/5fF7fP87+rD3Pj0l28P9Mty3zn8p/pPmIIXy/dznX3XM3qTUdo0q0//xzcln0bjtI52A9n9xGcPXga/5AoRveIx7IrhUOlNoxaiegu0UT27kRaqZ1T6aVYi9zRFIawWEIjA0gI8UdryseZk4Y5txxzvoVyp4pddTuyZA0YZAmjVsIv2A99sAFdoAEpIAiN3oBL649L64ddcWFXVMwOBatTxWJXsDtV7E4VxamiKCou1eVpqlrlKHOpbheOtyunhg9fOb/rRrh1zo+s0yHr9D6aru6TXGEIyQaBQCDwQeWkzFd/Y0MYfoFAIPCBRuPbx6/RNDrHiTD8AoFA4AuNJKHxsYDrq+9KpdGNeGtWGb+s/YiFd7zFPdtVPvzX9UTqtcx4YwdP3nMVmzJKifvHPPKP7ia6Uz+U9QuxKC4S77iNrdvT0AWGMKF3AhlffEOOzUnHYD2BfQaRLYVy9EQh1pJ89IEhhMUY6RhlRJN9jOLjmeTZFCyKOzTPqJUI18sYowPxi45EDQyn3K5SaHGSW2rDanFgtzmr/PsVIXWVVH51lCpeNZLsCe/USBpkWUKWJSSthKyVkCUN2gqfvl4rVfj73duV/nyp4j2AXNPJXg2pWqimj4950VBJdpcaww+w7NMjpO/ZwtrDefTdtZVnn7mHq8IM7Fq5io7b3uDW5Db8vCGFNyf1YGehhaYPzybj52+5aXBr+gYUs7vISs+keLacKqboTCohCR0Y1CKc9G9/JsfmJMZPS2zv1vh3SeLXrDIKs8qwm0qQ9QYMYdG0iQkiPtgPuSwXU0Ye5TkmzIVuH79SoX+olzQYZA3+/lr0gTr0QYHoggOQAoNR9QZUnQGHC+yqy+Pftzrd/n2L3YndqeJSwaW6PH5+92uVT19VFeGnbyiq/Z3W1hCuHoFAIPhzoakjnLMxzviF4RcIBAIfSFo9kk7vs7+xIQy/QCAQ+ODPOONvdCN+eF4yPcdNxOFy8cmS5bRJeZk7Zl3PqR83cFt4HuF6mfdOudAHhjBkeGf2vrqJDkF+FLW9gczUPUS07snA5iGc2nICxQUJPWNxJvZib2YZ+RnFqE47ARFNaJcYSkKwDseZ3yg5U0qeTUFxuf21gbJEQLiBgLhw5Ig41IAwyu0qBWY7hSa7J6VesVtQnQ4UZy0x/NX8+pLH5+/277vj+N1+e41G4/Hp67WSx9dfFcdfFbsPlbH8Vfs8rUKyAari+T1jqabTU31fY+CFNyfyyqJZzJo1gKue+Ip7ctYxccM8AiKasPKhD+j0xhJsZYUk/rySJv5aNhYGo9gtPHxdc4o/ep1yp0qH2wfyzvbTOEwlxLdrSnNNEWe3ncWiuGht1BHSvTuOuE7sOVNEWW4WqtOOPjCEoHADbWKNRBq0qLlplGfkYS6wUGhXsKouFJd3DL8uUI9fiB+64ADkwCCkwCBcugDQ+WN1urBXyDXYKmL4zXYFW7UYfqXC118Vz694WiXni+E/t//cYwS1U5nAdd52kQlcfyRixi8QCAQ+kCQJyceMX2qEM35h+AUCgcAXdSRwiagegUAg+JMhyTqfC7iSLCQbBAKB4E/Fn1GyodE5p5a3uIUfprTikTdvw88Yxlv/WI32H68S3LQtvzw0i9GDmvOfj/bRvO9A5gxuzXf7c+l3bVM+Ss3BlHeWFl0SMBz7gf1pJYTrZRKu78jxUhffHcunLPM4GknGGJNAj8RQQl0myo6eoDS9lFKnewGsMnkrMCaAwNhwtJGxKIZQSm0KBWY7BeU2bBYHDqsVp92dwFVTHEsjyR6BNo0se8LFJE/SlnuRV5I1ngQuvVY+V6BN8hZok72SuKoE2ryuXUOgrWYSlaTx1uGvvr/ymOrb7nP+cSvBM4zjSf7iOfbf8SLHvl3LoklLWOTsyT8euYXdRVbm/WKnxbU38eMjb3HTwESeX3OAiNY9aZaxg31v/0iCQYd+8GQO/pKFrDcwuFc86r6vOZZRhl7S0KRLNNqOfcmwyew7XYilKBsAv5BIQqMCaRUeSJBqxpl1ClN2MWUlNkocKhalStDPvyLpzy9Yjz7IH31QAJqAYDSGIFw6P1SdoUKgzYXZoWJzKm6BNsUt0KYqKqpTxeVyVSzwugXaqi/uViZzgfcCbnUt/gtBLPhW4XNhty430BWKmPELBAKBDzSye4Lmq7+xIQy/QCAQ+KCh9Pj/SIThFwgEAh+IBK4rgMUvv8tn3UayrvO9zH3yNjKtDsYt3cWEO2/iky2n6PHK05ze/iVTx3Um/vDnZFqddLp/NB9sOY6sN3D7dS3I3biWsxYHbY16wq67nh/TithzJA9LUQ5ag5GI2CC6RAehzT9J0dGz5JTZsSguZA0EVwq0xQQSEBsBQZGU2RTyzXbySm2UmdxFWBSbxV37tEaSTU2BNrcwW2URFqmi8HlVApcsafCrJspW2fTaiqIsFQJt4PbXVxZnqY5Gc64oW0MJtF3uIut18eHLr/Ps/K+4Y8ZSBk65B5Oi8vxzy3ksLpNx7SN4++3NvDjlaj4/kk/352Zx7IetdB/YjRNvLOPHk0Vc0y6cXy1B5B3ZS0jTttzcOY6sr77jtNlOpF4mrndzLOEtSc01kZ9Rhq2sCEmrJyAinuYxQSSG+iOXZmFOz6Qss5xCu+JVhMUt0CZh0Mv4hfihDw5EHxyIFBSKS2/ApQvAieQpwGJzKlgVdwJXpUCb4nRVJHC5vP37yrnCbOetp+ujCIvAN5JWi+yjSdrGN39ufCMWCASC35HKiZiv/gth48aNLF26FKfTyR133MGkSZM8fYcPH+axxx7zbBcWFhISEsJnn33G2rVr+c9//kNERAQA119/PTNnzrzAu3EjDL9AIBD4QFPxLdxXf33Jyclh4cKFfPrpp+j1eiZMmECfPn1o3bo1AB06dGD9+vUAWCwWbrnlFp5++mkAUlNTeeyxxxg5cuTF30wFjc7VIxAIBL8nUoXb1VerL9u3b6dv376EhoYSEBDAsGHDSElJqfWzb775JldddRW9e/cG4MCBA6xdu5bk5GQeffRRSkpKLv6eLvrIP4jYzv34Ns/MjDnvcb/tR+6+pQM/r13DK4OisasutmjaAXBX+0D2//stEgw6GPp3Tv+8j7DmnRnRNpLjG/dhUVy06RAJ7fux+WA2OWnFOK3lBEQ0oXmzEFqF+WM/vp/C4wVkW53YVRcG2e3fDwn3JzA2FG1UPEpgBOUOt0BbbpkNm8XpLsJi91FkvXoBlgo/f2WRdU/xFa2EJHsXXfHy9dcUYdO44/mhdoE2r+vXLJp+Cb+Pho7hr+v0ydPu467BLZD9DHwxGB5941acVhMpw6Yz6JOXKDy5jxHqQfSShl8i+mAuyOSZER356dPDZFuddLnrWt7aeQZzQSbxHdvRORTSvjtGiUOltVFPVFIPThTZ+OlMESU5+Tit5RUCbUY6xQcTE6DFlZtG2dlcTLkmShwqJkX1CLS5C/do8Av2c7dQI3KgESkgCFUXgKrzx6a4sKsubNUE2mwVRdbtdsWryLrqcuFyecfw11xHOp9AW22IeP260Uh4hBNrb+7PZWVlkZ6e7tVKS0u9zpWbm0tUVJRnOzo6mpycnHOuWVZWxqpVq5g6dapnX1RUFA8++CAbNmwgLi6O+fPnX/Q9CVePQCAQ+ECS3BXxfPUDTJo0iYyMDK++qVOnMm3aNM+2qqpekyWXy1Xr5GnDhg0MHjzY488HeOONNzzv7733XoYMGXLhN1OBMPwCgUDgg8qZva9+gBUrVqDUiJ4KDg722o6NjWXPnj2e7by8PKKjo88555YtW7jvvvs822VlZaxZs4Y777wTcP/DkH0kldVFo3P1CAQCwe+JbzdP1T+FuLg4mjZt6tVqGv5rrrmGHTt2UFhYiMViYfPmzfTv39/rMy6Xi4MHD9KjRw/PvoCAAN5++2327dsHwAcffCBm/AKBQNBQ1LWAeyGLuzExMcycOZPJkyfjcDgYP348Xbt2ZcqUKUyfPp0uXbpQWFiITqfDz8/Pc5wsy7z66qs8/fTTWK1WmjdvzksvvXTR99ToDP+XTw6kQDeU/31bygfjn2di5q/4jXyW4w9P4eZecTz8/l6aXT2E4reeYcv3aVzfM5aPUnMpTT9K71tuJSb3V9YdKSREJ5F4Q3vOOAI5cbyA4rNHAQiOa0mfVhFEae2UHT1CyZlSSp3uxTKjViLST0tgdCDG+CjkqHicAWGUFjnIqxBos1scOGz2CoE2xzkVkioF2iStzi3MVk2gTZYlTxUuSa5K4JIlDXq5qgpXpRhbpTgbVAm0VT6DNQXaKvdXLvRWCrRVuhc11Y51f+7cY2sTaGtI6rNu/J7fl5x5fz0bbA7e7tGP4O++5tagPDb+bTVp5a1I6DOCnfc/zchecfzj418Jbd6ZHvajvFdkJdZfS+i4e/lhwTEkrZ5re8aj2b+Z344WImsgsV0E+q792ZVewo5j+Zjy0oBKgbYAWkcEEiI5cGSdpjw9n/IiKyalSqBN1mjwl9zVt/RGHX7BfuiDA5CCwpACg3Hqq8TZ3AJtCmaHW6DN4nBX4KoUaFMUd1OdlZW4fAu0Vb5X1dqqc/le0BULvt5oNHgWcM/XfyEkJyeTnJzste+tt97yvI+IiGDbtm3nHNe7d2/Wrl17YRc7D43O8AsEAsHvSh1x/I2mTmk1hOEXCAQCH8hatzy6r/7GhjD8AoFA4IPLLdlwJdDoonp23jCGvbfM49En7mZfiY0bl+5izN1j+WjVIa5Z9hRHvvmCByd0Zed/vua02UGPmaP5b8pRNJLMbde3JG/dSo6W22hr1BM9+Aa+P11I3ql0zPmZ6AJDCI8LontcMLq84xQdPkN2sZVyp+oRaAuMCSCoiZHA+Cg0oTGUOTXklNvJLrZSUm7HZnHitJSj2CwoTrtPgTbvRC5NRdJWlUCbVivhp5XcRVhqiLTJmqpFJ1njLdBW3bdfKdBW+R58++i9CrVc4QJtAP+6/R3637OYiOencNrsYOqc5Szr7WR0YgjPLPyC5x7oy+qd6fR9dRapX31H1xuu5uTCBQD0bxPOIU0Tsg/uJbhpW27tGU/OF19ywmQnyk9LfL+WWKLb8eOxPHLTS7GW5FcJtMUF0ToiALkkA/Pp05RluQXaLEqVQJtBdhdgMfpp8Q/zxy80CL/QIKTAIFw6t0CbzalidapYHW6Rtssl0Obl6xcCbZfEOQmRtbTGhpjxCwQCgQ/qG8ffmBCGXyAQCHxRh+H/3cLcLiPC8AsEAoEPLmcc/5VCo/Pxb80q485//Jd/abbz9wkd2bVyFf8dHku5U+UrQw9cqsIDnYxsyTW5BdpumsrxXXsIb9mNmztEcWTNXiyKiw5doqHLID7bn0V5zmmPQFubFmG0DTdgP/Yr+UfyzhFoC4ozegm0lVQUWc8ts2E1O7BZHCh2C4rdWqdAm6zVewTaJK2ERqJeAm36inh/nSTVW6Ctpp+/ktoegPo+FFfCotYdQ1si6fQsfOtnHls2CVtJPpuuvYuhmxaRf3Q343ELtP0aNwBT3lkW3NyZHz86QM9Qf7r9fQCLtp7ElHeWhM4d6REGJ1IOUeJQ6RCkJ6ZfL44X2Th6soiijGyPQFtwZAhdE0KJCdBCzmnKzuZSnlVOoV3ForjqFmgzhqL6GVF1/lgrBNrcRVjc/n2zXalVoE2pKLh+MQJttcXmi3j9+iFJGndezfmaMPzn8uKLL3oKC2zfvp3k5GSGDh3KwoULG/rSAoFAcMlcTlnmK4UGNfw7duzwZJpZrVZmz57NkiVL2LRpE6mpqXz//fcNeXmBQCC4ZOory9yYaLAhFxcXs3DhQu6//4Y30nYAACAASURBVH4A9u/fT2JiIgkJCWi1WpKTk89bgEAgEAiuGCrrYJ+nNcbM3QYz/E899RQzZ870qNPVtwCBQCAQXElImjpcPcLwu/nkk0+Ii4sjKSnJs6++BQjqYsYzo3CpCsvGvkDcsk8IiGjCwbvvZMLA5sx6ezct+w0nb9GTyBoY3K8p7/yaRWn6UVr1bk/U2Z38criAcL1Mi+FdOG7158TRAqwleWgkmZD4VlzTJpJoyUxp6kGKTxZT5HAvgBm1ElEBOoKahhDULAZtbDPUwAhKrArZ5TZyS61YTXbsFgsOazmq8zwLuz4E2iqbJLuTudyibPJ5BNqqRNqqC7TJ0qULtFXncgu01fePpL6PRunij/nxrfuY1Dee99vfxcOP38NnWWW8kNWElv1Hs/X2OYwb1Jxp7+8lonVPuhTtZXeRhb43tyP4lgf5cdsZZL2BoX2bwZ7POHy8CFkDzTpHoe8xkB1ni8nPLPUItPmHxRAeE0i7KCMhGhuOs0cpS8ujpNAt0FaZvKWXNATKEiE6Gb9gPf6hBvxCjW6BNmMoLr3BXWVLcWFzVgm0lVud5xVoc7lc9RZoA7wE2ioRAm0XjlYr1dkaGw0Szrlp0yby8vIYPXo0JSUlmM1mMjIyvAoHnK8AgUAgEFxJVKrk+upvbDSI4X/33Xc97z/99FN++ukn5s2bx9ChQzlz5gxNmzbls88+Y9y4cQ1xeYFAILhsyBrfhl9uhK6e3y2By8/PjxdeeIFp06Zhs9kYMGAAw4cP/70uLxAIBBeFXMeM31fflUqDG/6xY8cyduxYAJKSktiwYcMlne/9xPG8ucCfvb2XM/iF7/nHI7fw/rC3WXBiHcfHL+X9/3uKbwb9k8FxQXR//C7u23gYrb+RBwa3IXPlXE6Y7PQNNxA19CZWncgn/9QpXKqCX1A4UU1D6N0kBDn7IAUHT5FZavMItIXpZIKaGDHGRxHYJBpCoim2q+SabGQXWykz2bFXCLSpDjtKjeStmgJtUkXyljuBqyJpq7LVkrTlSd7SSh5RNkmqStSqFGirTvWAg0r/fl0CbZLnfcMItF1uRt/1PF/Je2mTspmbkueSP78HP/WNZ8x/PmHjkvtZsTiXxWtf59A9axh593gOPTcXvaSh9UP3s608iKwDOwhr3pnbejYl/YX5nDDZaeKvo9mA9pSEtmLLVwcozTqJtSQfWW8gMKoZ7RJCaR0egLboFIWn0ig9W0ahXaHcWZU85U7ekjDoZQxh/viFBeEXFoQU5Pbvu3QBWGv4902VRVjsTix25RyBNlctAm21vVYvwCIE2i4dSfJt3KXG5+IXkg0CgUDgC62kQevD8Pvqu1IRhl8gEAh8oKuQT/HV39hofCMWCASC3xFZkupsF8LGjRu56aabGDp0KCtWrDin//XXX2fgwIGMHj2a0aNHez6TmZnJpEmTGD58OA888AAmk+mi76nRGf7FL7/LdZ8/xyPPJ3Nw0yc8FpdJmE5mUZoRfWAI44Jz2VZgoe+/hpHXfSynf9pOdKd+3NwhkkOrfsGuumjXNx5nx0Fs2JtBec5ptP5GjDHN6dI6glZh/tgO7iL/twKyrQqKqyKG308muGkQQc1ikGOaoQTFUGJTyDW5BdosZXZs1iqBtprFMABv3371AiyVsfsV4mvVBdr0nmIsksePf27xFbeP/UIE2ir9+RcrtHYxxzVEoktCr+tZ8UMafR/5jMCoBJaNmk+fL9ZiLsik+953STDoWFnaBFtZIS+N7MBXnx3nhmgjZxP68eJXR7GW5NGyZ3vaSgUc/+Io5U6VbqH+RF7XjwO5Zk6eKMRckIlit6APDCE0KpCuCSHEBmpRMo9TejrLU4SlMoZf1oBBdhdZN4T5VxRhMSIHhSIHhaLqjShaf2xOF1anWuHjrxJoM9sVlMr4fWdVgXXF6TxHoA2ots+3QFvNQuyC+iHVWGer2S4knDMnJ4eFCxfy4Ycfsm7dOj7++GOOHz/u9ZnU1FReeeUV1q9fz/r165k0aRIA8+bNY+LEiaSkpNC5c2eWLFly8fd00UcKBALBX4DKcM7ztooJTVZWFunp6V6ttLTU61zbt2+nb9++hIaGEhAQwLBhw86RrklNTeXNN98kOTmZ+fPnY7PZcDgc7N69m2HDhgHuoJlLkbwRhl8gEAh8UBnVc/4Zv/tzkyZN4oYbbvBq7733nte56pKuMZlMdOjQgVmzZrF27VpKS0tZsmQJRUVFGI1GtFr3smxUVNQlSd6IxV2BQCDwgaw5N1S6Zj/AihUrUGqEz1ZqlVVSl3RNYGAgb731lmf77rvvZvbs2UycOPEc9+ql1MMQhl8gEAh8UN+onri4uDrPFRsby549ezzbNaVrMjMz2b59O+PHjwfc/xi0Wi3h4eGUlZWhKAqyLF+y5E2jc/XEdu7HS09uYu/I2ST0GUHKsOlMnt6P/yz5mh7JN5L6z8eJ9ddivHMOz31zAnNBJn2ubYFm6wp2nS0lwaCjzdgkdmeZSTuSj62skIDIJoQ2bcZ1rSMJteRQuP8I+aeqBNqCtTIRYf4ENQ1DH5+IrklzbPogCswOskqtZBVbsJod2M0mHBbfAm0aSTpHoE2qSOByV95yC7TptRJ6rezlX9RrJY9AW/UFXo9oWzWBNk/iVo2F3UqBtupImpoLv97JXnUJtF3u5K0Lmcik/qs9c54bQc6BrWxcOJlMq4Nh//uNPhP+H6v+/n9MmHoNc9/eTbO+NxHx4zscLbfTa1p/Xv3hNPt/OIx/SBS3X98S+zcr+CWjDKNWIuHapkhdruf7kwUUZOTjMJUAEBDRhOj4YDpGGTHaCnGcPkzpmUIKS22UOt0CbZWVt9wCbRL+Yf74hwXiHxqEZAxFExCCyy8Qq1PFqqiU252U270F2ix2BadDQXWqqIoL1eU6p/JWdYG28y3UXmjylljwrZ3KOH5frb5cc8017Nixg8LCQiwWC5s3b6Z///6efn9/f15++WXOnj2Ly+VixYoVDBkyBJ1OR+/evdm0aRMA69at8zruQml0hl8gEAh+Ty5nVE9MTAwzZ85k8uTJjBkzhpEjR9K1a1emTJnCgQMHCA8PZ/78+TzwwAMMHz4cl8vFXXfdBcDcuXNZtWoVN910E3v27GHGjBkXfU/C1SMQCAQ+uNxaPcnJySQnJ3vtq+7XHzZsmCd6pzrx8fEsX778gq51PoThFwgEAh/8GdU5G52r58snB9I33MAdj69gw9whbEwvJfCJpeT9tpN37+jF+o3HuGlAM946UMimTQcJjErgnze05di7a8i0OukVZyRw0DhW78uk8NQhNJJMaEJbmrQI46r4YFwnfyZv3xnSzE4siope0niSt4Kbx6Fr0hwlOJYiq0JWmY30QgumMjs2iwOnpdydwHUegTa5InmreiKX27ev8SrAotdKaCt8+tVbZbKWTtKg8xRkqZpxVE/egiqRtvoItEH9H4ZLiSa43Cxsm8yq/o/wz/nTCP33FP7x7Ah2rPiQlPuvZmehhci5y0jbuYlHJ/dk2+PLSTDoiLxnFpu2HCf/6G6iO/bh5vaRHP14K2ctDloF6kkc3IN0TRjfpGZTlulOrtH6GwmObUrPxDBahPqjLTxDyYkMis+UkGdTsCju5Cm9pPEkbwUE+7kF2kKD8AsPQQ6JQPULRNUHYnHWEGizOzFXCLTZ7Aqq4sLpULwSuFyqglrxbKnV/PwALlU9J7HrfAhf/oWh0577t1i96bRXzt9DfREzfoFAIPCBkGUWCASCvxhSHa6exlhzVxh+gUAg8IGY8QsEAsFfjD9jzd1Gt7i784YxjPt1LeXZpwlZ9gijE0MY++YuYrsNJOarRWRanfR4ZgZLP0kl58BWWvZJojtn2bv5FAZZQ9tRHcgIasW2XzMx5Z3FLyic2MQwBnWKoXmQjPnAXvKPFJBvd6K4IEQnEeuvJaRpMIHN4nGFNUENiqbYqpBVbiOrxIKl3IbN4sBhLUd1OrwSuDyVt3R6z6tckbwlayW0Otm9sFuZxCVXWziSJa8qXDpJQlep4Fnx9dO9mFtLUhYar+StyveSRuOlzHlO8lXNtPA6fh/1febr+3X4Qr81R/nJPPaP//DPwtW8+uYeDtz8FOEtu3Hs7nGMaRnGbR/uIyCiCXcnOkk5WsCw65vxRb4/mfu24lIVrr22OeGnt5H641kUF3RoHUbw9SP4Ma2E7NPFWIpykPUGDGExRMYH061pCHEBEvaTByk5kUFZtokSR5UyZ/XkrUplTv+IYKSQCKSgUFS/IBxI2BS3KmdZteStcpt7gbdSlVNRVFyqq+K1KlmrevIW1L5YW7OvrgVdseB7fnwKtNXxT+FKRcz4BQKBwAdajXvS5au/sSEMv0AgEPhA1ribr/7GhjD8AoFA4AOpDlkG4eP/HdiaVcaA9zO4+9F7WfLCNwzdtIi9n67liQf6s3nmRwyODuRgk/6c/ukbNJLMA8kdyHlvCftKrHQL8afp+DF8cayArKNnUJ12guPbck3HaK5tHo4+8wA5e34jI9dMiUNF1kCYTiY4PojgFrFom7RwV95ySmSV2cgotFBQYsVqcuAwlaDYLCjO2pO3JElG0uqqKnBp9W7/vrbCvy+7m9ZTcUv2EmfTayVvUbZqfv7aBNo8Im3VvPTnezxr+6Za81mu77P9e/8JTDi7h8S+Q5l/xzuMah3ObY99yBtPjuGdTw4zePW/+W7lZySNHcbJp2ZhV110feI+XtxwCIephLDmnZl2XUsyV35EaqmNJv5aWg5pj6lpT75IzaIw7QSq045/SCTG2Ba0aRZKp2gjuoKTmI4fo+hkMdlWJ6VOFcVV5d83aiVC/LUV/v0Q/CNCkEIicBmCcfkZsThUrE4X5XYFi6OaQJvd6RZos6sVyVsuj59fddo9a0e1VdKqb/JWbQj/vm88IojnaSKcUyAQCP5kSHUYd2H4BQKB4E+G8PELBALBXwxZktDKLp/9jY1GN+IZz4xi98cf8GrzswTKEi9kNUFnMDIlMocvc0wMfmY0D6/8FaelnNhuA7mtcyT7392JRXHR/fpmOHuOYuWOMxSfPYzW30hMy3gGtomkc3QA1n0/krMvmzSzA7vqwqiViDdoCU0MJqRVPHJsC0waf4psChllVtKLzFjK7NisDpzWchS71SOiVYnHx+8RZavw7+t1VcJssluoTVtD/MmvukBbteIrlb5+uSI+v7o4m6TRePz61YuxVE5KqsfwV6f6g+DLn1/9uMsdw38xtLnvE/Y/3YcOQX5c//O3lGaeYMi+t2jir+N9pSPWknzeubUbGz5M5caEYNLa3shvW3cQFNeKNn270k2bx6FVv1LiUOkVGUDsTcPYnVnOod/yMOWdRSPJGGNaEBkfTp+W4SQE6VDSDlN87Cyl6WUU2r0F2ozaqhj+gAgD/hHBaEPDkYNCUfVGFK0/FqcLk12hzOakzO6k3OqO3zfbFezV4vgrxdkUp9Mrhr+6QJu7qV4/E18x/MKff+FU/s35ao0NMeMXCAQCHwgfv0AgEPzF+DNKNgjDLxAIBD6oTRKlZn9jo9H5+AUCgeD3xC3ZcP52oZINGzdu5KabbmLo0KGsWLHinP4tW7YwevRoRo0axYMPPkhJSQkAa9eu5dprr2X06NGMHj2ahQsXXvw9XfSRfxDvJ46n66hjvDP4Yaa9cSst/vMJI+68mR13/IO2Rj3KrXM4cMsCojv2Y9SN7VDWL2RreiltjXraThzCllPFnErNwmEqIbR5Z7p0iKZHnJGQklOk/3SArFPFFDncC2BhOpmIqEBCWkSjb9oSJSSWAotCdpmd9CILWcVWzOV2bGWlOCzlOO0WVKfdM1ZP5S2dHo0kIekqFnj9DN7CbFoJSa5eacst0KavJtAmVeiFaGWp2sJu7clbUDFLQeOVnHWOkJvGO3nrfAJtv1fy1sW4SstzTvFxm4Hctm81vZ7dxm0z7+b1u27h/oW30HnhV7QfMgrd8qc5YbJz/5KJ3Pv5YcqyTtDj5luZMbwdZete46fMMkJ0Eq2GtcTVbSiffZtJ3ql0HKYSDGGxRCREk5AYSo+4YAJNOViOplJ0LI+8EqsneUvWgFErEa6XCdfLGCINGCKDCIgOQwqOAGMELv8gLE4Vi1N1L+raK8TZKgTaLHYFp8PdVMWdvKUqao2FXMVLsO18iEXcy8fllGXOyclh4cKFfPrpp+j1eiZMmECfPn1o3bo1AOXl5Tz99NOsWbOGmJgYFi1axOLFi5kzZw6pqak89thjjBw58pLvScz4BQKBwAeSpu4GkJWVRXp6ulcrLS31Otf27dvp27cvoaGhBAQEMGzYMFJSUjz9DoeDuXPnEhMTA0C7du3IysoC4MCBA6xdu5bk5GQeffRRzzeBi7qniz5SIBAI/gLUV7Jh0qRJ3HDDDV7tvffe8zpXbm4uUVFRnu3o6GhycnI822FhYQwZMgQAq9XKf//7XwYPHgxAVFQUDz74IBs2bCAuLo758+df9D01OlePQCAQ/J7UN5xzxYoVKIq3iy04ONhrW1VVL5eqy+U6x8UKUFZWxkMPPUT79u25+eabAXjjjTc8/ffee6/nH8TF0KAz/kWLFnHTTTcxYsQI3n33XcD9VSc5OZmhQ4de1OLE4pffZfvDXThcZmNb0kOYCzL536hEPt6RztgHk5ix/hDlOacZOqIbjw9qyd5XN1FoV+jTLQbtDZN5b+cZik7uQ9LqiW7VluGdYog0Z+JM3UbW7tOcMjmwKO7krVh/d/JWWNsEdM3aYvELI7vcTkaplTMFZkylNqwme0XylqX25C25KoFL9iRxaSv8+5qqJK4aSVuVBVgqC7Lo5KrkLV2ln1+qkbhVkbzlJdJWkbxV+XDW9gtvyOSthuanDx7lhMnBde9nc/jL1Sxpl0uRQ+HY8FnkHtrG/x66ho1zP6NvuAFl7D/54Yu9GMJieXBEe0Ym+nPgf1vJtDrpGepP4qhBHC6T2LYvi7KsEwAERiXQpFko/dpE0jLUD036IQp/O0PRqWKyrQrlzqrkrcoCLAHhBgIiAzBEhaELDUUOi0L1D0LxM2JyqFidqtu/X5G8VVYp0GZ14nRUT9xSUVWX57mqmbwF4FJV7z7lwvz+Yi2gbmSp7gYQFxdH06ZNvVpNwx8bG0teXp5nOy8vj+joaK/P5ObmMnHiRNq1a8dzzz0HuP8R/O9///N8xuVyIcvyRd9Tgxn+n376iZ07d7JhwwbWrFnD8uXL+e2335g9ezZLlixh06ZNpKam8v333zfUEAQCgeCS0Wokd/W78zStpv5m9JprrmHHjh0UFhZisVjYvHkz/fv39/QrisL999/PjTfeyBNPPOGZbAUEBPD222+zb98+AD744INLmvE3mKvn6quv5v3330er1ZKTk4OiKJSWlpKYmEhCQgIAycnJpKSkMGDAgIYahkAgEFwSlzNzNyYmhpkzZzJ58mQcDgfjx4+na9euTJkyhenTp5Odnc2hQ4dQFIUvv/wSgM6dO/Pcc8/x6quv8vTTT2O1WmnevDkvvfTSRd9Tg/r4dTodr732Gu+88w7Dhw+vc2FDIBAIrjRkCXxotHlcPfUlOTmZ5ORkr31vvfUWAF26dOG3336r9bjevXuzdu3aC7vYeWjwqJ7p06ezY8cOsrKyOH36dL0WNnwR27kfX3UbzsxZ13PvvPX0mfD/OPz3OwjXy8Q9+RpfrtlKeMtuzB3ahtAdK/hufy4JBh1d7hnIjgKJ/XszsRRlY4xtTruOUVyTEILzwFbyd+wm+1A++faqGP64CANhbaLwb94KJaQJBRYnZ0sspBVbSC80Yy61YTOVYzeVoNit5/j3q/z6OmQ/g1uoTadHkt1F1rU6Ga3e/aqvVoBFL0teBVgqowo8RdY1VBRfP38MP5wbF6/x7Nf8bjH8DVVkvZIzAwcx+5sX2fvJCvrdcScfDHqYhx4ZwK0vfEviNcm02/0uOwst3Pivwcz96gT5R3fTou+1TGgfivPzJWxPzcOolWh/fSLapDGsTc0m81gG1pI8/ILCCU9IoF+bSHrFhxDqKMJ29BeKjmSRl2+hqKLIelUMv4QxzJ+ASAOB0UEYIkKQw6KRgsLdMfwOdwx/SUXcfrnN7d+vfK2M4Xc6Kgqtu1ye4iuq017l31e8/fy+ED78S0dTmxBitdYIpXoazvCfOHGCw4cPA2AwGBg6dCi7du2qc2FDIBAIriQqgyV8tcZGgxn+9PR05syZg91ux2638/XXXzNhwgROnTrFmTNnUBSFzz77zGthQyAQCK40fMXwV7bGRr18/Pn5+axcuZLi4mKv/XPmzDnvMQMGDGD//v2MGTMGWZYZOnQoI0aMIDw8nGnTpmGz2RgwYADDhw+/tDsQCASCBkTCt8uzMWbB1svwz5o1C39/fzp27HhBPvlp06Yxbdo0r31JSUls2LDhwkYpEAgEfxR1uXMa34S/foY/OzubL774oqHHUi++fHIgjzxfSvQDr5B/8z858c7tzPnnEe67tyePfHmG4tOp3Pzw/UTv+Zif//0BmVYnE69ugmHkvbz5xSnyjuxF0uqJat2RMd3jiXPkkbNtJ5m7TnC83EG5U8Uga4g3aAlrGUp4++bom7fHHBhFdq6ZtGILJ/NMlBZbMZfZcJhKUOwWnLZaBNpkGUmr8yzyynoDWr0fWr18TvKWQS+7F3ZrVOGqTN7yqAFWJG/p6kje8lThAp8LULUlb9X20SsxeQsg5WgB/94dTdJtk9kyJphHH7XSd8Yi0obPIOWTF/m83xy6hfgTPP1lPp2yEr+gcO4b1RHls9f59Y0vOW120C/CQJtbBnLUGcrmvccozTgKgDGmObHNQ0lKDKNDZADSmV8o2H+CgmNFZFudXslbwVq3OFtgdCABkQYMUWH4RUcih0WjGkJQ/YIwWxQsDpUSm5Myu0KJ2eFe5LU6sNkVr+StytdaBdrqSN6qb6KWWPitHxIan7N6qRFa/np9S2nSpAlms7mhxyIQCARXHH/Gxd16zfijo6MZM2YMV199Nf7+/p79vnz8AoFA8GegugLn+fobG/Uy/PHx8cTHxzf0WAQCgeCKozJ3xld/Y6Nehn/q1KmYTCYOHjyI0+mka9euGI3Ghh5brey8YQyzZg3g2sc/pe/ESRy7exxGrUSzl95m9Z3vEd6yGy+P6sjPNz/Klp8ySTDo6H7/YHaWBbJ7VzrmgkyCm7alY5cY+ieGov7yCRnbj5OTmkeOzQlApF5LXISBiHbRGFq1QQlLINfs5HSR279/Jt90Aclb+gtI3nL79P2q+fjPl7wl+SjAApVJJ94/O4n6JW95Pn+FJ28BPP/N8xinv4t1zYO83+tWHn6kP0Pmf02zpJEkHfqImbkm5j4/gse+OEZO6lY6DBvPXV2j2PvoJn74JRuDrKHroOborp/Aml+zyDjqTvDzCwonIrE5AztE0yEygAhnEbZDP1FwOIPcXBP59tqTtwJjAgmMdhdgkcOikUIicRpCMDtdmKolb5VaHe7krYpXu82J6lQ9yVuKorqTthx2kbz1B1OXO6cR2v36Gf79+/fz4IMPEhkZiaIo5OTksGzZMnr27NnQ4xMIBII/FAnwodjw5w3nfPHFF1mwYAF9+/YFYMeOHbzwwgusWrWqQQcnEAgEfzSaOmQZLlR25kqgXv+sTCaTx+iDOxbfYrE02KAEAoHgSqG+pRcbE/Uy/BqNhoyMDM92enr6JRUBuBS2ZpVx/N4F5B/dzeYpXXnnk8NMvv9q7tt4ksKT+xg+7jqitr/H5p8yybQ6ub5nLP5jHuTV746Te3g3klZPTJtO3NKrKfH2LHK/+5HMA7kcKbNT7lQxaiXiDVoiWocR0akl+padsAZGkVFq51ShmTP5FxbDL/sZ6h/DL9c/hl+W8BnDX70Ai3vfuVyOGP4/+pkftD2GgVPu4e1et5FaaqN4+iLObN/IR48NZPXdS7kqzJ+A6QtYvWoH/iFRPDy+M47VL/Pdz9mcNju4KsxA24lDOewIYdOusxSfTgUgKK4V8S3DuLZ5OJGOAqSzqeT9eoz8IwVkWM4fwx8YHVQVwx8RWxXD71BrjeEvszo9MfxOh+IVw1+9AEvNAut1xfBX9++LGP5LQ0Md4Zx/9AAvgnq5eh566CH+9re/kZSUBMC2bduYO3dugw5MIBAIrgTqClL400b1DB48mJYtW7Jz505UVeX++++nVatWDT02gUAg+MOpy5Xzp3P17NixA4DNmzdz/PhxIiMjiY6O5sSJE2zevPl3GaBAIBD8kWjq0RobPmf8n3/+OUlJSSxfvvycPo1Gw9ChQxtsYAKBQHAl8Gec8fs0/M8++yzgVufs2rWrV9/27dsbblQ+mPHMKFr+4/8Y/dDd7E0eQxN/HaHz32bDLQuI7tiPV5Lbs7P/A2RbnbQK1NPj4WQ2Z2v4eedZLEXZhDbvTM+ecQxIDMXx4zrO/nCMI2X2aslbMvHRAUR0bIKhdXuc4c3IMTk5XSHOVlJowVRqw1pagt1UgsNqOmdht7oom+fVz1CVuFUtecugl/GrWOAN0HuLtLkXciW0suRJ3tLJVQu6tSVvVS7yesajqUrcqvzM5UzeOh+/R/IWwO5VH1L26g08Y3Hw+MJxdH9sHR2GjadNysv8X4GFF9++jQfWpJL32066jZnAba30/Hj3Js5aHIToJHqMbI088Hb+991Zzh48ibUkD/+QKKJaJDKsSywdowLgyHYsh38m/0A62Xlmr8pbITqZcL1EUEQAQU2MBMRGEBgXgRwRhyY4EiUgDJPTRblDpdDioMTqpMhsp9jsoNhsx1Kj8lbl+1orb6m+k7dqW9gVXAY0Gt+z+gt8iDdu3MjSpUtxOp3ccccdTJo0yav/8OHDPPHEE5hMJnr37s28efPQarVkZmYya9YsCgoKaNGiBQsWLCAwMPDC74c6XD2HDh3i4MGD/Otf//K8P3jwIPv27ePp15VaygAAIABJREFUp5++qAsKBAJBY0LW1N3qS05ODgsXLuTDDz9k3bp1fPzxxxw/ftzrM7NmzeKpp57iyy+/xOVyefKl5s2bx8SJE0lJSaFz584sWbLkou/J54z/o48+Ytu2beTm5jJ16tSqg7RahgwZctEXFQgEgsaCpNH4zNy9kASu7du307dvX0JDQwEYNmwYKSkpHvuakZGB1Wqle/fuAIwdO5bXXnuNW265hd27d/PGG2949t92223MmjXrou7Jp+F/5plnAFi4cCEzZ868qAsIBAJBY6Yuu17Zn5WVhaJ4u9mCg4MJDg72bOfm5hIVFeXZjo6OZv/+/eftj4qKIicnh6KiIoxGI1qt1mv/xeLT8O/YsYOkpCQ6depUaxTPH7G4+//bu+/wqMq08ePf6TPJJIGQShICoQbpRYqaCCgQkgBmcUUsq+zy6v4ooisKyOrrqy5FbCjYFnVlESuCsAFsFAWXJlIUAYEAKSSTQurUM+f3x2QmmZRJVAgZeD7XNRczZ87MOQfCw+F+nvu+342fhL3yn6y5Af7fvWdZ9PoUJv5zL5Wmc8z+220o1zzNf46Y6BWs47oR8SjSZ/Hca3so+Gknar2RuF49mTIojogLJ8j64huyfiok1+LA5pQJ0SjpFKghvGcYYX06o+7Ui1JNG84WV/KLqYKsggoqLliwVNmwV5XisFR6kmzclGotKo0WlVaPUuOK7yvVWtRaTXV8X+n5VVsd0zdo1V7JWwatypO85f6vpEal9IrrqxpJ3gK8krca4yt5S9nIHEBzk7dacl3zk0sf4ambxzL/wwdY034ipn/9g72vPMebMbNJjw3GNP5RtvxpGUHRnXl6cj9K3nyKrceLCNepGBIaQMKfbuNbk8zXu89y4dxRFEoVIXGJ9OgRRnLHdrStOEfFgf9S9OMpCo8Vk2N2UGp3/XkbVEqC1Uoi9RqC2hsJjGqDMSYcTbsw1GFROAPa4tAaqahyUG6VKLU4KLXaazVgcdTE9m0SDpsrgUtyODzF2eomb7keDSdvNUQkb/1+ClkGufF7fvdP+x133OGV6AquApe1uxA6nU6vv1OyLHu9buz9uvvB7ysVIVb1CIIg+CI7fQ787qF/9erVDd7x1xYVFcW+ffs8r00mExEREV7vm0wmz+vCwkIiIiIIDQ2lvLwcSZJQqVT1PvdrNWtVT92B3263o9FofvNBBUEQ/IbsdN31N8o18EdHRzf5VcOHD+fll1+muLgYg8HA559/7gmpg6v3iU6nY//+/QwcOJD169eTlJSERqNh0KBBZGZmkp6ezrp160hKSvrNl9SsWj379u1jxYoV2Gw2br31Vs8JCIIgXPGcTnBKPh7OZn9VZGQkDz74IHfffTcTJ04kLS2NPn36MG3aNA4fPgzA0qVLWbhwIWPHjqWqqoq7774bgCeeeIIPP/yQcePGsW/fPmbPnv2bL6lZJRueffZZHnjgAb788kvatGnDf/7zH2bPns24ceN+84F/q5effZtlz/6Vj4aNYVyUkRNj57Dntifokjyeef0MrL9zPTanzE23JtJ52j3884fzHPvuR+yVpUT0vI7RQzuQHB9C1UevcmbrKY5X2DxrstvrNUR2CCG8T0f03fvhCEsgr8LBiaIqfs4ro6zETGWZBXulaw2/w+ZdnE2p1nrW8bvi+gZPExbP2n1tzVp+rVrpWbuvq9Nk3VWYzbWGX6NS1lvD727E4inMVmcNf+3ibO7mK1Czrh+8Y4SXIwflYkwFTN78NLuCdDwmj2TlI68x9r57KJl9O7kWO7M/XsINr++mPO8kY/46jZvVWax//mtMVok/9GhHjz/0wzI4g9c/OkzOj66fEWNkR9p3jWFc72gSw/RI3+0mf9/PFB0r5GyxmUKbhCS7i7MpCdepCIwMICjaiDEmHG1kNMq2ERASgTOgLRU2iQqbaw1/mdVBaZWdC2bXGn6r1YHdWrOGX5KcON0NWNzr+Btssl4T33dr7hp+Ed//DWSn63GRpKenk56e7rXtzTff9Dzv0aMHH3/8cb3PxcTENBh2/y2adccvSRLDhw9n165d3HTTTcTGxuL8Ff/KCYIg+CuF7Gzy4W+aNfA7nU4OHTrEtm3bGD58OMePH8dut1/qcxMEQWgFnDV3/Q098L+Bv1mhnvvvv5+//e1vTJo0ibi4OEaOHMljjz12qc9NEATh8pNl36Ee2f+K9TRr4B89ejSjR4/G4XBgt9v54osvLlsjFkEQhBbVVIzfDwf+ZoV6ioqKmDZtGv369aNPnz7ce++9vytr7PeI6nUdqTtfZEdhFWnbX+f2RVvRGIysmD6Mk/Nm8WVBJaMjjXSbN58zCaN4/bOfKD51kIB27ekyqAtTBsSg/ekrTm3czU9nSjFZHWiVCkK1KjobtUT1iyS0bw8UcYkUOjQcL6rkp9wyck2VlBebsZaasFYU47BUIlnNnskyhVKFQqlCrTOg0updnbe0rodGp/VO3tIq0VUXZTNo1Riqi7XVTOy6JnTd3beUCkX1BK+iepuypgOXu9tWrUne5vwY1i7aVltLJW9drByvRUu2M+mXHax4+hU0gcF8MkrLq+8eZuqtibyvGcSh/3xG+4FjWD6pNz89/iRbTVUkBuno/9cbaf/n6Xz0YwH792RTnnsStd5Iu869ua5vNNd1aIM+9xDFu3eTf/A8+ceKyLU4MEuuAcCoVhKuUxMeoic4NhhjTBiBMeGowmNQh8cgGdpiUeooszldSVtWB0VVNooqbJRW2aiw1J7YrXlIDocnWUuqnuStnSQo15lb+7XJW8Kvp5AcTT78TbMG/v/7v/+jb9++7Nq1i127djFo0CBRpE0QhKuDO9TT6MPXGv/WqVkDf1ZWFjNmzCA4OJi2bdsya9Yszp49e6nPTRAE4fJzl2zw9fAzzRr4HQ4HVqvV89psNv+uOhGCIAj+4ypd1ZOamso999xDRkYGCoWCTz75hDFjxlzqc2vQlr+PYIHx/zHrLwOYdTiIs9+tJH3mfQw9vYHFq4/QXq/mhqcmsFPRmTe2HCNrj6thTHTva/lzcmd6KIvJ3/gZZ745R1aVHUmG9no1MQY1EX3CiRzYA13Pa6lq04EzBVUcM1W4kreKzFSVlmGrKsVhrsBurqjfgEWjRanWoNTUJG/VJG4pveL8hur4vlZVk7xVuzibK4HLFduveV6rSJvSuzibsjrq7i7OVjd5y5PYVev38mIXZ7scHpk9nN4LviV28GjeeOgGPh16I4lBOrq8vZZx095HpdHy6F+GEPbFy3yw/gQqBSTf3JGQyTP5WQpl5Zd7Mf28D6fDRtuOvYhPDCftmkg6KEqx7PuK87tPkH3qAuctDoqrk7cMKgVtNSqi9CqCY4MIiW9LUIdI1JEdULVrj9MQgjOwHeVmiQqbRGGVnRKzneIKG6VmOxeq7FjNdhw2CbvVVZjN6XBW/2rzSuBqTnG2hpK3RHG2i0fR5OTuFTjwHz9+nC5dugCwc+dOnE4nGRkZTJo06ZKfnCAIwmXnlMDpYwLXD6MfPgf+Tz75hMWLFxMfH8/Zs2dZunQpN9xwQ0udmyAIwuV3td3xr1q1ig0bNhAZGcmBAwd44YUXxMAvCMJV5UoM9TQ5uRsZGQlA//79KSkpueQn1JT/jppI3xAdPP0O/3rpXToMS+O9W7vz5b0vkG91cMvEbihvf4wFnx5m66aDVBXlEprQl9FJnUjvFop92/uc2HCIHy5YqHA4CdEo6WLUEN8hmOhBnQjsMwBHZHeyy+38VFDBjzmlFJsqqbhgxlJqwl5Z1miDdbV73b7eiNpgRKPXo9WpUWtUaHRqNNXPDZ41/DW/euL7zSjOVrv5Su3ibEpF/fh+Q5qamP+tE/ctvYYfYF3GU5zd8wX7loyl02sPsdVUyb0fziHl1d3kH9nBoIzxTIutZOvDazhZaWN0bDA9H/4fvi4J4PntJzm9/wjmkvMEtGtPTM+uTL42jsHtjXD0W3K/OcD5H/I5XWkn3+rwarAepVfRJtpIcGwQQR0i0cfFoY7qgGQMQwpsR5nNSZnN6YnvF1ZYKaq0eRqs260SNqvD1YClVoN1dwOW+g3Wa+L7tTU3vi/8Dk5n0w8/43PgrzsA/Nps3VdeeYXU1FRSU1NZsmQJ4Oo5mZ6ezujRo3nhhRd+5ekKgiBcBlfQUk5o5nJOt1/bVPjbb7/l008/Zd26dfz4449s3LiR+fPns2LFCjIzMzly5Ajbt2//1SctCILQYnwmb13cks0txWeM/9ixYwwYMMDz2mKxMGDAAE//x++//77Rz4aHhzN37ly0Wi0AnTt3Jisri/j4eOLi4gBXXerNmzeTnJx8Ma5FEATholM4Hb5X9Th/1f1zq+Bz4P/iiy9+8xd37drV8zwrK4tNmzZx55131uswf7lq/giCIDRLk9U5/S/c43Pgj4mJ+d0HOHHiBPfddx+PPPIIKpWKrKwsz3sNdY5vyo68ch7b/z6d529CpdWzZu4ITtx/Bxuyy5iY0JaeC//BnC9OcuSrnVTkZxEYHkfidb24b1g8gT9+zo8fbuPIiWLyq4uzdQzQEpcYRrvuYYRd2xdlQn/OS3p+LCjjh3OlnM4pp7zYjLmkAHtlqSdxq3ZxNqVa22BxNrVGhUavQqWqKc5m0KvrFWdzF2jzJGzVSeCqXZxNo1J4F2Rrojibe5+6XbcaS96q++fRWouzuc2bvYjnXpnP98Nv5JMjBcyc2o9/hdzE7veX0GFYGmvuHciRqRlk5pTRK1jHsPmp5HUbw6JV33P2ZxMlWUdQ641EJg5i5KBYRiWEEpj9Pee3byf7v+c4WWKh0ObALLn+codoVERWF2drEx9CSKcoguLbo46MQw6JxBnYDrOsoswsUVRlp7DK5lWc7UKFDZvFgb1W9y1JciI5HEhWV0e3usXZ6k7s1k7eqquxiV0x4fs7uFss+nrfzzQrc/e32r9/P7NmzWL+/PmkpqayZ88erw7yv7dTvCAIwqUmy02s3LlIMf7c3FzmzJlDUVERnTp1YunSpQQGBnrtU1BQwLx58ygsLESpVPLII48wbNgw7HY7Q4YM8YTRAdauXdvogpxLFpzKy8tj+vTpLF26lNTUVAD69u3L6dOnOXPmDJIksXHjxt/VKV4QBOGSu4jN1n158sknmTJlCps3b6ZXr16sWLGi3j5Llixh5MiRrF+/nueee46HH34YSZI4duwY/fv3Z/369Z6Hr1WYl+yOf+XKlVitVhYtWuTZNnnyZBYtWsTMmTOxWq0kJyczduzYS3UKgiAIv18LhHrsdjt79+5l+fLlAGRkZHDnnXcyZ84cr/1uvvlmhg4dCkB8fDxWq5WqqioOHz5McXExGRkZqNVqHn74Ya699tpGj3fJBv4FCxawYMGCBt/77LPPfvP3zn5qPKPWXiDvwJc8tuhhum5+lqc/PEqvYB03Lv8rH5WG8/GnX1Oe52qs0XHwUB4e3Y3ullNkvfc+P+04x/EKV/JVnEFD9w7BxA7vTNvEeLS9r6fUGMPx/EoO5ZZxNKeUC6ZKKotLsJSasFWVIdnMXvHSuvF9d/KWtlbCllqjQqtTodOpMWhVGPUaDJqa5C2tWolepUSnVrkStqpj+0pF/eJsCgWoahVe8zRiwXdxttp8FWdraD+31hbfBxjwh8nc8vkinjpcQHpsMOqFq3hs6nIC2rVn+QPXo3h9Lp/85xdCtSrG3tUX/Z2PMS/zF37eeYiyvJMAtOsygH4D23NbvxjibLmUf7OJc9t/Juv0Bc6Z7Z74vlGtJEyrIi5AQ9uENoR0CiOkcwzq9p1QhnfAERRJmaSiyu6kxOygsMpGYZUNU5mV4kpXnN9mdbgSt+xOr+Yr7vh+Q8XZgAbj+w0lbzVExPd/H9nhAF89xhWuYTQvLw+pztxLcHAwwcHBTR6jpKQEo9GIWu36rvDw8AYXvtQujrly5UoSExMJCgpCoVAwatQo7rvvPk6cOMG0adPYsGEDoaGhDR7vksb4BUEQ/F4z7/jvuOMOcnJyvN6aMWMGM2fO9Nq2adMmFi5c6LUtPj6+3sIKXwtf3nnnHT744AP+/e9/A65oilvPnj3p06cP33//PTfddFODnxcDvyAIgg9yMwf+1atXN3jHX1dKSgopKSle29yTs5IkoVKpfC58WbJkCdu3b2f16tVERUUBsG7dOgYMGECHDh1c5yzLaDSaRk/Z/zIPBEEQWpJTbqJWjysUGB0dTWxsrNejOWEeAI1Gw6BBg8jMzARcA3lDC1/eeecddu/ezZo1azyDPriSbd966y0ATp06xdGjRxk4cGCjx/O7O/534yex69mnueHee5nX5hgvPvQxBpWC2/53HMf73MZTS3eQf3gHxsiOtOvcm/vH92RUhETB62/y89qfOFhqweaUaa9X0yvMQNx1HYi44Vo0HROxRffkZIGZ/TmlfH+mhKLz5ZQXl2G+cB57VZlXc3VwxfeVGm29+L5Gp0WjU6PVqarj/Eo01fH9oDrxfYNWhV6t8l6/7y7WpvJev+9uvlI7vq9oRny/dsG25jZfaa7LGd8H2D7iArOGb+GR2cOJffx5Eud9TlVhLo89PYPkk5/w+tOfU2p3cufNneg4/ymW7T/Pps0/U3zqILJTIjShL10GJnDP0Hh6B9mwfbWBrC37OXeogJOVNkrtrhUbBpWCMK2KDtXx/dAu7QjpHIMurhPq9gk4QqIwK/VcqJIwO5zkV9ooqHTF9wvKrRRVWLGY7djMrjX8rji/hMNmdTVWqY7vS17r92vW8IMrvu8mmq+0HFlu4o5fvji/x0888QRz587l1VdfJTo6mueffx6ANWvWUFBQwKxZs1i+fDlGo5G77rrL87k33niD6dOnM3/+fNLS0lAoFCxevBij0djosfxu4BcEQWhRDgc4fEzuqnyUc/gVYmJiWLVqVb3tt99+u+f53r17G/38smXLmn0sMfALgiD4IDslaCRTGhCZu4IgCFccUbJBEAThKuPO3PX1vp/xu4H/5WffpmfKJLZMimBNn/8h12Jn+v2Dsd3zNNNe3sWpnZvRBYWSeOP13NS/PX/qE4H5vWf48d97+K+pklK7k1Ctit4hOuKTOhAz8lrUfZJwtInllxIr+3JL2Xe6mLycMkoLq6gqysFWXuIpzuamVGtR6QyNdt3SGdRoDRp0BjUqlRKjXk2QXt1g1y29WolOrUSnUqJSKtB5JnqV9bpu1U7igoa7btWesG2oE1dD86y192kooavhAnCXd2IX4O/Jc/jTiHhO3P8itz7/Pdl7NzNx5jTmRufy8Y0vc7Tcyh97RzDw+cf5yGTkzbX7yT+8A6VaS0C79nQa2ItpyQmMiA/G+c17nN30Lee+zeanMivFNtdf9hCNkkCVkg4BGsLiggnt2pY23eIITEhA06EbUnAUVl0IxVUOisx2Si0OCiqt5JdZPBO75ZU2rGYHVosdu0Vydd6y2Ws6btWZ2HUlctUUZ2sqcUtM7F5CTieyj8FdIQZ+QRCEK0wLreppSWLgFwRB8EF2OJB9repxXJxVPS1JDPyCIAi+iMndyy+q13V8/uhQtvRM4r/FZu6/rSeRS/5F2qu7OfJ5JkqNlm7JI3nyD70ZFB2I87MXOfDqV+w6VYLJKhGiUdI3REdCUgc6jBmMbvBoLoR0wlTpYPe5Enb9UkjW2VJK8iuoNJ1tML7vab6iNaDWB6INDEETGII2IBCdXoPWoPYkcOl0arRqJUF6NUa9hiCdGqPe9TBoVOjcDVg8TVfwStryJG8pajVgoSaOr6oT3/eco8I7LbuhuH1DzVcudnz/UhvZsQ0h720g9Z4XqcjP4ro/3cPqmwLZPOwvbDVVMSE+hOtff5QvVT1Z+O4+zvz3S2SnRMQ11xERH8E9o7qQ3r0dyn3rOfvZFk5/eZqDFyzkWx1IsqswW3u9hlCtkujYIMK6hxKaGI+xaxc0HROR2sRgDQyn2CxRVOUgr9xKmdVBXqmFvFILBWUWSitsWCrtWM2u+L7N6sButSFZzZ6if5KjJmlLxPdbGbmJyd0rreeuIAjC1U6WGu94Bvhe499KiYFfEATBF2cTHbjEqh5BEIQriyzZkWuFeuuRfEz8tlJi4BcEQfBBbmIdv7jjbwFb/j6Cbb2HsSG7jPsmdKPL22tJe2Mv+z5djyxJdB+Zwv9O7scIbS7WLV+w/8WN7PypkFyLo3piV0/3G+JISBuCYXgape26cTC/kqwLZrYfN3H8lKsqZ6UpG2tpIbbKUiSb2XN8ldaAQqlCYzCi1gdW/2r0mtjVVSdu6Q0ajHo1OrXSa2LXoFV5JnZdnbdc3bc8FTkbmdhVKalJ4qo+n7oTuzWVO93veyd01a3Keakndi/1HHDXXdu59t7lKJQqrp18F59PjuGrpFvZkF1GWnQQI995hO8ikpn71l5++fYLJJuZiJ7XMezG7tzYI4LbrglHd2AD5z5exy+bTnDQVFVnYldNZ6OWgDAD4T3DaNerI8E9uqLt2ANnu3jsQVEUVTkorHKQXWbhfIWVUrOdvAuuid3iMiuWKtfErs3sqDex63TYcLqrc9ptYmK3FZIlGVnyMbhXd2jzJ3438AuCILQk2en0PfCLO35BEIQriwj1CIIgXG0k33f8Cl//G2il/G7g/++oiXz2SzF/vTWRzu+sJeXV3ez9ZB2yJJF40zj+cecAbtJmc/rZReTuzWbboQJPfH9AGz09bownIW0IAUkTudCuGwfOV7Ltl0LOFFVy/FQJhblllOefaTS+r9YZXMlbjSRu1Y3vtwnQuhK4Gkjcqh3f11cncikVimbF9+sWZgPf8f3aofYrJb4PMOjOF1BqtHy07H9IMhTyxdAM1p8pJT02mJvfX8COiBE8vHIPx7duRrKZieydxPUje/DIqK50aqPD8P16zn6wlhMbj3HQVMU5s90rvt8tSEf4NWEERgQQ3jfBFd/v0gdnWEfsQVGYqhwUVNrJLrOQU24hp9hMucVBXqmZ4jIr5gqbz/i+O3FLxPdbL8nuwGlrfOWObBclGwRBEK4oTYV6fIaBWikx8AuCIPjQ1OTuxRr4c3NzmTNnDkVFRXTq1ImlS5cSGBjotU9OTg5paWl06NABgLCwMFauXIksyyxZsoStW7eiVCp56qmnfDZbVzb6jiAIguBZztn44+Is53zyySeZMmUKmzdvplevXqxYsaLePkeOHCE9PZ3169ezfv16Vq5cCcCWLVs4efIkmZmZLF++nHnz5uHwUTXU7+74d+SV8+D/uxbDU28zaum3HPrPOjQGI73TxvDilP4MqDjIsSee49sNJzhntmOySoRqVQxuq6f76AQ6pt+AdlgqpqCO7MsuZ8cvhew+bqKyzEpRXjkV+ac98X13YTZPUTadqyibUq2tF9/XB2o8jVd0OjVtAjQY9RqMOvc6/pr4fkB1jN/VcEXpie9rVNUx/lrx/dqNV5SKhuP7NTH7+jF/+PXx/cZC862h8UpdhrZRfPXiZNRP/YVPPjzCVlMVf+wdQfL7S/jI3pUnV3xH1q4tALQfOIabb+rCQ8kJdDWfwr5zP6c+3MAvmSf5vsTsWb8folESZ9DQua2e8J5hhPWOJTAqlKDu3dB26YPUNg5LYDimSjsFlXbOllrIq47v55W6YvwXyq1YKu1YqmwNFmZzOmw4bGZXHRhRmK1Va4lQj91uZ+/evSxfvhyAjIwM7rzzTubMmeO13+HDhzl+/DgTJkwgJCSExx57jO7du7N9+3bGjRuHUqmkU6dOREdHc+DAAQYPHtzg8fxu4BcEQWhJstOJ09eqnuqBPy8vD6lOwbbg4GCCg4ObPEZJSQlGoxG12jUkh4eHk5+fX28/nU7H+PHjmTx5Mt988w3Tp08nMzOTgoICIiIiPPuFh4dz/vz5Ro8nBn5BEAQfnDYJp63xsImiukXnHXfcQU5Ojtd7M2bMYObMmV7bNm3axMKFC722xcfH12+R2sB/nWt/V3JyMs899xynTp3C6XR67S/LMkpl45F8MfALgiD4IMtNhHqq6/GvXr26wTv+ulJSUkhJSfHaZrfbGTJkCJIkoVKpMJlMXnfwbqtWrSItLY22bdtWH1tGrVYTFRVFQUGBZ7/CwsIGP+8mJncFQRB88D2xW7PiJzo6mtjYWK9Hc8I8ABqNhkGDBpGZmQnAunXrSEpKqrff3r17+fjjjwHYs2cPTqeThIQEkpKS2LBhA5IkcebMGbKysujdu3ejx1PIsuxXFYbK3lvCT9f9hT8t3cHpbz8jKLozwyeOZFlGL9of+pSDS97h253ZHK9wTcy216u5tn0QXdO6E5MyEtWA0WQr27E7p4yvj5n48VQxhTllmMvLqSrKwVpa6NVxS6FUeRK33ElbSrXWNbFrMKAzuCZ2dQYNWr0ag957YjdI7+rAZdSr0VcnarkndvVq1+Sua5LXNamrUoJKqahJ1mrGxK77f3i+Jna9CrVdIRO7bgfGjObdHWcxqBTcndaVXq+/zpIjdl5ftZv8wzvQBobQYVAyt6V048+DYok8u5O8jz6g8Mg5ju/K5kiZFZNVQqWAUK2KOIOGTlGBhPcMI7xPR9r27IyqXRSajj1xtI2lQh1MkVkit9xKTpmFnDIL2cVmzpeaKSyz4rBL1RO79upJXQd2i8UzsSs5bJ6ibO7JXDGxe3HFR7fjROazF+W7chY/jHShqNH3VW3aEfPo0t9/nJwc5s6dS1FREdHR0Tz//POEhISwZs0aCgoKeOCBB8jPz2fu3LmYTCZ0Oh3PPPMMPXr08Czn3LFjBwDz5s3j+uuvb/RYItQjCILggyzLOH2EepQX6d45JiaGVatW1dt+++23e55HRkby9ttv19tHoVDw6KOP8uijjzbrWGLgFwRB8MFps/ss2eDrvdZKDPyCIAg+yLLcxOSuX0XLgUs88FdUVDB58mRee+01YmNj2bVrFwsXLsRqtZKSksKDDz74q7/z3fhJ/N+cKuYsAAAWAUlEQVTsd7iQdYTo/jfxP3cM4uGh0VS9+ww7ln3BjtMXMFklwnUqInVq+vYMo8v4foSPHofUI4mjpU52nClk69ECsk6XUJxfQUX+aRzmCqzlJUg2sydeqlRrUekMqLUGNIHBaPRGNIEhqLSG6mJs1YXZ9K7EraAADUF6NSEGLUHVDVeM1TF+V0G26uJsau+4fu1fa8f03QXavJ7TcNIW1CnWVuv37EqO738c25+dRWYmD4wm8Y+DkO9bxIQ1B9n92VbK804SFN2ZHknDmJnSnYld28CO1Zz4YCMnNp8ix+zgZKWNCocTrVJBpE5Np0ANsQltXIlbfToTlJiINuEanAFtsLeJpURSU1Th8BRlyy4xk11ipqDM4knactglrGYHNrPrud1ShWStSdpyx/YbStoCPEldIGL7rYHcRHVOn7X6W6lLtqrn4MGD3H777WRlZQFgsViYP38+K1asIDMzkyNHjrB9+/ZLdXhBEISLormrevzJJRv4P/zwQ5544gnPWtJDhw4RHx9PXFwcarWa9PR0Nm/efKkOLwiCcFE4qzN3G32I6pw1nnnmGa/XBQUFhIeHe15HREQ0mJIsCILQqjh9x/hxihh/oxpKKW4oJbkpLz/7NpYLNgbfdqenKNvxGQ/zzbrjHCy1AJAYpKN/YjvCurfzFGUrDOrIvrMVnqJsBdlllJ0/71q7X17iWlfdSFE2VzE2V1E2nUGDSqX0WZQtSF/TdEVfXYitsaJsnoJsSkXNen2xdr/ZcswOHn8qBd0Dz/HFqRKe/N+vPEXZYgaP8yrKVvz6Uo5/so9DR0ycrLRhlpyNFmUL69MZbcI1qGK7YW/bAZtSW910xVqvKFveBYunIJvV7MDpcPosyuZ0N19x2D0xerF2v/Vy2iWctsZ/r512//tzaLGBPyoqCpPJ5HndWEqyIAhCa+IO6fh639+0WMmGvn37cvr0ac6cOYMkSWzcuLHBlGRBEITWRHbKTT78TYvd8et0OhYtWsTMmTOxWq0kJyczduzYljq8IAjCb+J0yjh9NFtxioG/vq+//trzfNiwYXz22WeX+pCCIAgXT3UHLl/v+xu/y9yN6nUds9N68Ofuei6s/F82v7SVHfkVlNqdROnVDA4LoEtKFzqMH4W2Yw9sXa7joMnC9kPn2Xq0gOwzFyjOK6HSdLZeQTaoSdrS6ANRG4yeSV13QTadQY1ao3Ilbhk0GPVq2gRovSZ1DVoVgVq1pyCbayK3JmnLNbmr9HTcqt1pS4k7iavpSV2ok8zlvoZGJnXrvlf7M977tP5JXbeHj63jtXMBPPe3TC5kHabSdI42HXvRM2kgD4/twej2KqStK/npgy849vUZjpRZOW9x1VY3qFxJW12MWiIT2hDRO5KwPp0J7NYDbefeONrGUq5tQ6FZospu42yphfPlFs6VmMkrtZB3wUxZddKW1WLHanYVZJMcTlcxttpF2UTSlt+SnbLP9ooi1CMIgnCFkWwSko9VPb7ea63EwC8IguCDU5Z9xvGdolaPIAjClUWWmgj1iBj/pbfl7yOw/vspdvx5C9tPFmOySoRqVYyJDKTrqI4kTEz2JGyZqhx8+8N5tv1cwMnTJRTllVNpOoulJB9bZalXwpY7aUtjMHoarrgStwLR6TWehC2tToVao/IUZDPqNQTpahK2DBoVARqVV8JW3WQtT9JWndi+qjpw31RBtrrJWA0VZPOVsAX+H9t36/3cSU/ClqFtJAP+MIXp43pwa2I7+OY9Tj23gV8yT/J9iZl8q8MrYStUq6J9fAiRvcNpd00ngnv2QJPQC2e7eCoDw10JW0WuRiulVgfZtWL77oJsliobdovklbDlTgZsqiCbSNjyD7Ik+1yrLwZ+QRCEK4yY3BUEQbjKiMldQRCEq4yrEYuPO34xuXvp7bpxAlv2H/es20+PDfas21cPGkueNpK9OWVs3X2SM0WVPtft147rK9WaRtftu4uxBRg01Q1W1D7X7ddupF43vl933X5LFmOr/Zna/DG273Z271bih47mltFduS6hXfW6/VWceLb+un13I/WEiAAieoYREGFsdN1+3vkqcsot5FY3Uq+wOhpdt+9upO4pxOaURGz/CuKK8V/6yd3c3FzmzJlDUVERnTp1YunSpQQGBnrtc//995OXlwe4Cl8eP36cjz/+mB49ejBkyBDi4uI8+65duxaVStXgsfxu4BcEQWhJchOZuxdr4H/yySeZMmUKqampLF++nBUrVjBnzhyvfV577TXP85deeol+/frRu3dvjhw5Qv/+/Vm5cmWzjtViRdoEQRD8kXtyt9FHdRgoLy+P7Oxsr0dZWVmzjmG329m7dy9jxowBICMjw2ejqlOnTrFu3ToeffRRAA4fPkxxcTEZGRn88Y9/ZM+ePT6PJ+74BUEQfHA2Eepxv3fHHXeQk5Pj9d6MGTOYOXNmk8coKSnBaDSiVruG5PDwcJ+NqlasWMGf//xnjEYj4Arxjho1ivvuu48TJ04wbdo0NmzYQGhoaIOfFwO/IAiCD5JDQvLRbEVyuN5bvXo1kuS9X3BwcL39N23axMKFC722xcfH16ur1VijqtLSUnbu3OnV5XDy5Mme5z179qRPnz58//333HTTTQ1+h98N/N8WVNA5UEvfQdF0Th9A25vSsXYayhGTmW3Hith69BC5Zy9Qcv4C9spSqopysFWWIVVPukHDhdiUai26oDY1XbX0mkYLsWnVSs+kboBGhb66w5avQmzuiVyV0nchNsAzqXspC7G59vPfSV23j/85l1FRChxfvUvx+8f49tODHM4qJavKhlmSMagUdAzQ0MWoJbprKBG9Iwm9phPGHj1RtY2A6C442sSSZ4Uis4Oz+eXklFnIvWAmu8RMQZmFsnIrDrsTS6XNM6FrszoaLcRWu7uWKMR2BWgic9ddnTM6OrpZX5eSkkJKSorXNrvdzpAhQ5AkCZVK5bNR1fbt20lKSkKn03m2rVu3jgEDBtChQwfAtdJIo9E0eg4ixi8IguCDK8bvbPxxERK4NBoNgwYNIjMzE3AN5I01qvrhhx8YNGiQ17Zjx47x1ltvAa74/9GjRxk4cGCjxxMDvyAIgg/uGL+vx8XwxBNP8OGHHzJu3Dj27dvH7NmzAVizZg0vvfSSZ79z584RGRnp9dnp06dTXFxMWloaDzzwAIsXL/bE/xvid6EeQRCEltRSRdpiYmJYtWpVve2333671+s333yz3j5Go5Fly5Y1+1h+N/DPfmo8xmuTKI/uy6H8KnacLmLr1/swZZdRcr6IyoKz2CpKGmywojYY0egD0QSGoNEb0QSGoA8MQGvQoFIrvOL6IQEagvQaQjxJWyqMejV6tcoVw6+O6+vUKjTK6ph+7QJsSoWn8FrtImzNSdSCXxfXb25MH5oX12/NMf26IubcyQffZXO03EaFw4nN6Yrrt9dr6B6kJaxbKBG9owjr0wVDt2tQxycitY2lXGWk0u6k2OzgbJYrrp9TUhPXLy+3YqmyYzO74vkOm4TDLuGo/rmqHdd3JWlJIlHrCuWUZZ+ll0VZZkEQhCuMXZax+Yjjq8XALwiCcGWRZBnJx+Du673WSgz8giAIPjhl3/3U/bAqs/8N/O/GT2LD53mcP7u10Zi+QqlCpTV41uo3FNPXVcfytXo1xgANWrWSIL0Go05NmwCNV0zf3VjFHctXKpqO6bdk8bUreZ1+U976zwlCtSo6B7oaq0R2b9doTD/L7OB8uY2c0xZyynIprbI3GtOvXXzNnQPSnJh+Y3F8EdP3X+KOXxAE4Soj7vgFQRCuMjYnPid3bY0X7my1xMAvCILgg5Mm7vhb7EwuHjHwC4Ig+CBi/K3Ay8++zekzNaVPVVoDap0BQ9tIrw5aOoMOtUblScjSGdToGyi45i62pq1OyKqdmKVX1++g5Z7EVSjwFFy73IlZzZ3EdX1/s3f1CwtX3o2+Wy9Usd1xGkKwGiMpMkucqLRzttRM3nkrOT8Vkl1yloIyK5XlNqwWO5ZKO06Hs3oi1+aZxHU6bCIxS/AiNRHjv0iJuy3K7wZ+QRCEluRs4o5fZO4KgiBcYcQdvyAIwlXG7vRdssHuh+s5/W7gj+p1HZouDnQGdU0iVnXSlbFOUTWtWkmgVo1eXR23V7mapTQUu1cqFJ5GKc1JwAIabKACIgGrJd2vSqfggBXLt8U47CYsVT9ht0hYLXYcNrsndu+ojt/LkiRi98KvIiZ3BUEQrjIyvpds+t+wLwZ+QRAEnySauOP3w6FfDPyCIAg+iMndVmDL30cg+2FMTbg0Pn7h1ct9CsIV7kpcznlZeu5u2LCBcePGMXr0aFavXn05TkEQBKFZbNWrenw9LqYXX3yRl19+ueFzsdmYM2cOKSkp3HLLLZw8eRIAWZZZvHgxY8eOZdy4cezfv9/nMVp84M/Pz+eFF17gvffeY926dXzwwQf88ssvLX0agiAIzeKuztnY42KN++Xl5cyfP5+333670X1WrVqFwWBg06ZNzJ8/n3nz5gGwZcsWTp48SWZmJsuXL2fevHk4HI5Gv6fFQz27du1i6NChtGnTBoAxY8awefNmZsyY0azPK+DqXr8oeImPbne5T0FohWIi2l607wqOifIZxw+OiQIgLy8PSfJe4hscHExwcHCzjvPVV1/RsWNH7r333kb32bZtGw888AAAgwcPpri4mNzcXLZv3864ceNQKpV06tSJ6OhoDhw4wODBgxv8nhYf+AsKCggPD/e8joiI4NChQ83+vE6nuxSnJfipE5nPXu5TEK5wD33zcZP7WCwWJkyYQGlpqdf2GTNmMHPmzGYdZ+LEiQCNhnmg/vgZHh7O+fPnKSgoICIiot72xrT4wO90Or2SmWRZ9notCILgb2w2G2vXrq23vaG7/U2bNrFw4UKvbQkJCbzzzjtNHqfueCnLMkqlssFxValsPJLf4gN/VFQU+/bt87w2mUxe/1IJgiD4m18T0klJSSElJeU3HScyMpKCggI6dOgAQGFhIREREURFRVFQUODZz729MS0+uTt8+HC+++47iouLMZvNfP755yQlJbX0aQiCIPid5ORk1q9fD8C+ffvQ6XS0b9+epKQkNmzYgCRJnDlzhqysLHr37t3o97T4HX9kZCQPPvggd999N3a7nUmTJtGnT5+WPg1BEAS/sGbNGgoKCnjggQe46667ePzxx0lNTUWr1bJkyRIAxo4dy6FDhxg/fjwAzzzzDHq9vtHvVMgiG0oQBOGqclkSuARBEITLRwz8giAIVxkx8AuCIFxlxMAvCIJwlfGLgd+fi7pVVFSQlpZGdnY24CpZkZ6ezujRo3nhhRc8+x09epSMjAzGjBnDY4895rPOxuX0yiuvkJqaSmpqqmdFgT9f00svvcS4ceNITU311Ejx5+txW7x4MXPnzgX8/3ruuusuUlNTmTBhAhMmTODgwYN+f02XndzKnT9/Xh4xYoRcUlIiV1ZWyunp6fKJEycu92k1yw8//CCnpaXJ11xzjXzu3DnZbDbLycnJ8tmzZ2W73S5PnTpV3rZtmyzLspyamiofOHBAlmVZnjdvnrx69erLeeoN2rlzp3zbbbfJVqtVttls8t133y1v2LDBb69p9+7d8uTJk2W73S6bzWZ5xIgR8tGjR/32etx27dolDxkyRH700Uf9/mfO6XTK119/vWy32z3b/P2aWoNWf8dfu6hbQECAp6ibP/jwww954oknPBl0hw4dIj4+nri4ONRqNenp6WzevJmcnBwsFgv9+vUDICMjo1VeY3h4OHPnzkWr1aLRaOjcuTNZWVl+e03XXnst7777Lmq1mqKiIiRJoqyszG+vB+DChQu88MIL3H///YD//8ydOnUKgKlTpzJ+/Hj+/e9/+/01tQatfuBvqKhbfn7+ZTyj5nvmmWcYNGiQ53Vj19JQ4aXWeI1du3b1/KXKyspi06ZNKBQKv74mjUbDsmXLSE1NZdiwYX7/Z/T444/z4IMPesoH+Pv1lJWVMWzYMJYvX84777zD+++/T25url9fU2vQ6gf+K6moW2PX4m/XeOLECaZOncojjzxCXFyc31/TrFmz+O6778jLyyMrK8tvr+ejjz4iOjqaYcOGebb5+89c//79WbJkCUFBQYSGhjJp0iSWLVvm19fUGrT61otXUlG3qKgoTCaT57X7Wupub6rA0uW0f/9+Zs2axfz580lNTWXPnj1+e00nT57EZrORmJiIwWBg9OjRbN68GZVK5dnHn64nMzMTk8nkKQ9cVVVFTk6O314PuOrR2O12zz9msiwTExPjtz9zrUWrv+O/koq69e3bl9OnT3PmzBkkSWLjxo0kJSURExODTqfztEtbv359q7zGvLw8pk+fztKlS0lNTQX8+5qys7NZsGABNpsNm83GV199xeTJk/32et5++202btzI+vXrmTVrFiNHjuSf//yn314PuLpSLVmyBKvVSkVFBZ9++ikPPfSQX19Ta9Dq7/ivpKJuOp2ORYsWMXPmTKxWK8nJyYwdOxaApUuXsmDBAioqKrjmmmu4++67L/PZ1rdy5UqsViuLFi3ybJs8ebLfXlNycjKHDh1i4sSJqFQqRo8eTWpqKqGhoX55PQ3x95+5ESNGcPDgQSZOnIjT6WTKlCn079/fr6+pNRBF2gRBEK4yrT7UIwiCIFxcYuAXBEG4yoiBXxAE4SojBn5BEISrjBj4BUEQrjJi4BdahezsbBITEz0VGNPT05k8eTKZmZm/63unTp1KcXExACNHjuTw4cMX43QFwa+1+nX8wtVDr9ezfv16z+ucnBzuueceVCoVY8aM+U3fuXPnzot1eoJwxRB3/EKrFRMTw6xZs1i5ciU2m41//OMf3HLLLYwfP565c+dSUVEBuO7kn3vuOTIyMrj55pt57733AJg3bx4Af/rTn8jLywPggw8+ICMjgxtvvNGrjrsgXE3EwC+0aj169OD48eO88cYbqFQq1q5dy2effUZERARLly717FdaWsonn3zCqlWrWLZsGceOHWPhwoUA/Otf/yI6OhpwZbKuXbuWjz76iLfeesvzD4IgXE1EqEdo1RQKBXq9nm3btlFeXs6uXbsAsNvttGvXzrPflClTUCgUREVFccMNN7Bz5066d+9e7/vS0tIAV8nesLAwioqKPP8oCMLVQgz8Qqt2+PBhunXrRkVFBfPnzyc5ORmAyspKrFarZz+1uuZH2el0olQ2/J/Z2vspFApExRLhaiRCPUKrdfr0aVasWMHUqVO5/vrrWb16NTabDafTyd///neef/55z77r1q0DIDc3l507d3qqMqpUKtF3VRDqEHf8QqthsViYMGECAEqlEp1Ox0MPPcSNN97I0KFDWbx4MbfccguSJJGYmOhpJg6u5aAZGRlYLBYWLFhAQkICAGPHjuWuu+7i5ZdfvizXJAitkajOKfi9kSNH8tJLL9G7d+/LfSqC4BdEqEcQBOEqI+74BUEQrjLijl8QBOEqIwZ+QRCEq4wY+AVBEK4yYuAXBEG4yoiBXxAE4SojBn5BEISrzP8Hks3uLByfK0AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_position_embedding(position_embedding):\n",
    "    plt.pcolormesh(position_embedding[0], cmap=\"RdBu\")\n",
    "    plt.xlabel(\"Depth\")\n",
    "    plt.xlim((0, 512))\n",
    "    plt.ylabel(\"Position\")\n",
    "    plt.title(\"Position Embedding\")\n",
    "    plt.colorbar()  # 配色版，右边那个柱状\n",
    "    plt.show()\n",
    "    \n",
    "plot_position_embedding(position_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 padding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=208933, shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 1., 0., 0., 1.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_data.shape : [batch_size, sequence_length]\n",
    "def create_padding_mask(batch_data):\n",
    "    padding_mask = tf.cast(tf.math.equal(batch_data, 0), tf.float32)\n",
    "    # [batch_size, 1, 1, sequence_length]\n",
    "    return padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "x = tf.constant([[2, 0, 1, 6, 0],\n",
    "                 [1, 2, 3, 0, 0],\n",
    "                 [0, 0, 0, 1, 2]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 look ahead mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=208941, shape=(3, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_weights.shape: [3, 3]\n",
    "# [[1, 0, 0],\n",
    "#  [4, 5, 0],\n",
    "#  [7, 8, 9]]\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask # (sequence_length, sequence_length)\n",
    "\n",
    "create_look_ahead_mask(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"\n",
    "    - q: shape == (..., seq_len_q, depth)\n",
    "    - k: shape == (..., seq_len_k, depth)\n",
    "    - v: shape == (..., seq_len_v, depth_v)\n",
    "    - seq_len_k == seq_len_v\n",
    "    - mask: shape == (..., seq_len_q, seq_len_k)\n",
    "    Returns:\n",
    "    - output: weighted sum\n",
    "    - attention_weights: weights of attention\n",
    "    \"\"\"\n",
    "    \n",
    "    # matmul_qk.shape: (..., seq_len_q, seq_len_k)\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        # 使得在softmax后值趋近于0\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # attention_weights.shape: (..., seq_len_q, seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(\n",
    "        scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    # output.shape: (..., seq_len_q, depth_v)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "def print_scaled_dot_product_attention(q, k ,v):\n",
    "    temp_out, temp_att = scaled_dot_product_attention(q, k, v, None)\n",
    "    print(\"Attention weights arg: \")\n",
    "    print(temp_att)\n",
    "    print(\"Output is: \")\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_k = tf.constant([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=tf.float32)   # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=tf.float32)   # (4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights arg: \n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is: \n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q1 = tf.constant([[0, 10, 0]], dtype=tf.float32)   # (1, 3)\n",
    "np.set_printoptions(suppress=True)\n",
    "print_scaled_dot_product_attention(temp_q1, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights arg: \n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is: \n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q2 = tf.constant([[0, 0, 10]], dtype=tf.float32)   # (1, 3)\n",
    "print_scaled_dot_product_attention(temp_q2, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights arg: \n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "Output is: \n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q3 = tf.constant([[10, 10, 0]], dtype=tf.float32)   # (1, 3)\n",
    "print_scaled_dot_product_attention(temp_q3, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights arg: \n",
      "tf.Tensor(\n",
      "[[0.  1.  0.  0. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is: \n",
      "tf.Tensor(\n",
      "[[ 10.    0. ]\n",
      " [550.    5.5]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp_q4 = tf.concat([temp_q1, temp_q2, temp_q3], axis=0)   # (3, 3)\n",
    "print_scaled_dot_product_attention(temp_q4, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    理论上：\n",
    "    x -> Wq0 -> q0,  x -> Wq1 -> q1, ...\n",
    "    x -> wk0 -> k0,  x -> Wk1 -> k1, ...\n",
    "    x -> wv0 -> v0,  x -> Wv1 -> v1, ...\n",
    "    \n",
    "    在不同场景中，x(输入)是不同的，比如在self-attention中，输入（以下所述中\n",
    "    的q,k,v）相同，但是在encoder-decoder连接中的attention输入是不相同的（\n",
    "    以下所述中的k,v相同，但是q不同），所以在实践中，直接将输入分成三种q,k,v\n",
    "    \n",
    "    实战中：\n",
    "    q -> Wq0 -> q0,  q -> Wq1 -> q1, ...\n",
    "    k -> wk0 -> k0,  k -> Wk1 -> k1, ...\n",
    "    v -> wv0 -> v0,  v -> Wv1 -> v1, ...\n",
    "    \n",
    "    实战中技巧：\n",
    "    q -> Wq -> Q -> split -> q0, q1, q2, ...\n",
    "    k -> Wk -> K -> split -> k0, k1, k2, ...\n",
    "    v -> Wv -> V -> split -> v0, v1, v2, ...\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        \n",
    "        self.WQ = keras.layers.Dense(self.d_model)\n",
    "        self.WK = keras.layers.Dense(self.d_model)\n",
    "        self.WV = keras.layers.Dense(self.d_model)\n",
    "        \n",
    "        self.dense = keras.layers.Dense(self.d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, q, k, v, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # q.shape: (batch_size, seq_len_q, d_model)\n",
    "        # k.shape: (batch_size, seq_len_k, d_model)\n",
    "        # v.shape: (batch_size, seq_len_v, d_model)\n",
    "        # 其中 q,k 的d_model应该是相同的，它们与v的d_model可以不同\n",
    "        q = self.WQ(q) \n",
    "        k = self.WK(k) \n",
    "        v = self.WV(v) \n",
    "        \n",
    "        # q.shape：(batch_size, num_heads, seq_len_q, depth)\n",
    "        # k.shape: (batch_size, num_heads, seq_len_k, depth)\n",
    "        # v.shape: (batch_size, num_heads, seq_len_v, depth)\n",
    "        # 其中 seq_len_k 应该与 seq_len_v 相同\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        # scaled_attention_outputs.shape: \n",
    "            # (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape: \n",
    "            # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention_outputs, attention_weights = \\\n",
    "        scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        # scaled_attention_outputs.shape:\n",
    "            # (batch_size, seq_len_q, num_heads, depth)\n",
    "        scaled_attention_outputs = tf.transpose(\n",
    "            scaled_attention_outputs, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # concat_attention.shape: [batch_size, seq_len_q, d_model]\n",
    "        concat_attention = tf.reshape(scaled_attention_outputs, \n",
    "                                      [batch_size, -1, self.d_model])\n",
    "        \n",
    "        # output.shape : (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60, 512)\n",
      "(1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 256)) # (batch_size, seq_len_q, dim)\n",
    "output, attention = temp_mha(y, y, y, mask=None)\n",
    "print(output.shape)\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 feedforward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feed_forward_network(hidden_units, output_units):\n",
    "    # hidden_units: units of hidden layer.\n",
    "    # output_units: units of output layer. (= d_model)\n",
    "    return keras.models.Sequential([\n",
    "        keras.layers.Dense(hidden_units, activation=\"relu\"),\n",
    "        keras.layers.Dense(output_units)\n",
    "    ])\n",
    "\n",
    "sample_ffn = feed_forward_network(2048, 512)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    x -> self attention -> add & normalize & dropout\n",
    "      -> feedforward  -> add & normalize & dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = feed_forward_network(dff, d_model)\n",
    "        \n",
    "        self.layer_norml1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norml2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        # x.shape            : (batch_size, seq_len, dim=d_model)\n",
    "        # attn_output.shape  : (batch_size, seq_len, d_model)\n",
    "        # out1.shape         : (batch_size, seq_len, d_model)\n",
    "        attn_output, attention_weights = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output,\n",
    "                                    training=training)  # dropout层在训练和预测                                                         # 时模式是不同的\n",
    "        out1 = self.layer_norml1(x + attn_output) # 约束了x和attn_output维度相同\n",
    "        \n",
    "        # ffn_output.shape: (batch_size, seq_len, d_model)\n",
    "        # out2.shape      : (batch_size, seq_len, d_model)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layer_norml2(out1 + ffn_output)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "sample_input = tf.random.uniform((64, 50, 512))\n",
    "sample_output = sample_encoder_layer(sample_input, False, None)\n",
    "print(sample_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 60, 512)\n",
      "(64, 8, 60, 60)\n",
      "(64, 8, 60, 50)\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    <step1>:\n",
    "    x -> self attention -> add & normalize & dropout -> out1\n",
    "    \n",
    "    <step2>:\n",
    "    out1, encoding_outputs -> attention -> add & normalize & dropout -> out2\n",
    "    \n",
    "    <step3>:\n",
    "    out2 -> ffn -> add & normalize & dropout -> out3\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = feed_forward_network(dff, d_model)\n",
    "    \n",
    "        self.layer_norm1 = keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6)\n",
    "        self.layer_norm2 = keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6)\n",
    "        self.layer_norm3 = keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "        self.dropout3 = keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, encoding_outputs, training, \n",
    "             look_ahead_mask, padding_mask):\n",
    "        # x.shape: (batch_size, target_seq_len, d_model)\n",
    "        # encoding_outputs.shape: (batch_size, input_seq_len, d_model)\n",
    "        \n",
    "        # attn1, out1.shape : (batch_size, target_seq_len, d_model)\n",
    "        attn1, attn_weights1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layer_norm1(attn1 + x)\n",
    "        \n",
    "        # attn2, out2.shape : (batch_size, target_seq_len, d_model)\n",
    "        attn2, attn_weights2 = self.mha2(\n",
    "            out1, encoding_outputs, encoding_outputs, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layer_norm2(attn2 + out1)\n",
    "        \n",
    "        # ffn_output, out3.shape : (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layer_norm3(ffn_output + out2)\n",
    "        \n",
    "        return out3, attn_weights1, attn_weights2\n",
    "    \n",
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "sample_decoder_input = tf.random.uniform((64, 60, 512))\n",
    "sample_decoder_output, sample_decoder_attn_weights1, sample_decoder_attn_weights2 = sample_decoder_layer(\n",
    "    sample_decoder_input, sample_output, False, None, None)\n",
    "print(sample_decoder_output.shape)\n",
    "print(sample_decoder_attn_weights1.shape)\n",
    "print(sample_decoder_attn_weights2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 37, 512)\n"
     ]
    }
   ],
   "source": [
    "class EncoderModel(keras.layers.Layer):\n",
    "    def __init__(self, num_layers, input_vocab_size, max_length,\n",
    "                 d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = keras.layers.Embedding(input_vocab_size,\n",
    "                                                self.d_model)\n",
    "        # position_embedding.shape: (1, max_length, d_model)\n",
    "        self.position_embedding = get_position_embedding(max_length, \n",
    "                                                         self.d_model)\n",
    "        self.dropout = keras.layers.Dropout(rate)\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(d_model, num_heads, dff, rate)\n",
    "            for _ in range(self.num_layers)]\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        # x.shape: (batch_size, input_seq_len)\n",
    "        input_seq_len = tf.shape(x)[1]\n",
    "        tf.debugging.assert_less_equal(\n",
    "            input_seq_len, self.max_length,\n",
    "            \"input_seq_len should be less or equal to self.max_length\")\n",
    "        \n",
    "        # x.shape: (batch_size, input_seq_len, d_model)\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.position_embedding[ : , :input_seq_len, : ]\n",
    "        \n",
    "        x = self.dropout(x, training = training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoder_layers[i](x, training, mask)\n",
    "            \n",
    "        # x.shape: (batch_size, input_seq_len, d_model)\n",
    "        return x\n",
    "    \n",
    "sample_encoder_model = EncoderModel(2, 8500, max_length,\n",
    "                                    512, 8, 2048)\n",
    "sample_encoder_model_input = tf.random.uniform((64, 37))\n",
    "sample_encoder_model_output = sample_encoder_model(sample_encoder_model_input,\n",
    "                                                   False,\n",
    "                                                   None)\n",
    "print(sample_encoder_model_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_decoder_model_output.shape:  (64, 35, 512)\n",
      "decoder_layer1_att1.shape: (64, 8, 35, 35)\n",
      "decoder_layer1_att2.shape: (64, 8, 35, 37)\n",
      "decoder_layer2_att1.shape: (64, 8, 35, 35)\n",
      "decoder_layer2_att2.shape: (64, 8, 35, 37)\n"
     ]
    }
   ],
   "source": [
    "class DecoderModel(keras.layers.Layer):\n",
    "    def __init__(self, num_layers, target_vocab_size, max_length,\n",
    "                 d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.max_length = max_length\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = keras.layers.Embedding(target_vocab_size,\n",
    "                                                d_model)\n",
    "        self.position_embedding = get_position_embedding(max_length,\n",
    "                                                         d_model)\n",
    "        self.dropout = keras.layers.Dropout(rate)\n",
    "        self.decoder_layers = [\n",
    "            DecoderLayer(d_model, num_heads, dff, rate) \\\n",
    "            for _ in range(num_layers)]\n",
    "        \n",
    "    def call(self, x, encoding_outputs, training, \n",
    "             look_ahead_mask, padding_mask):\n",
    "        # x.shape: (batch_size, output_seq_len)\n",
    "        output_seq_len = tf.shape(x)[1]\n",
    "        tf.debugging.assert_less_equal(\n",
    "            output_seq_len, self.max_length,\n",
    "            \"output_seq_len should be less or equal to self.max_length\")\n",
    "        \n",
    "        attention_weights = {}\n",
    "        \n",
    "        # x.shape : (batch_size, output_seq_len, d_model)\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.position_embedding[ : , :output_seq_len, : ]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x, attn1, attn2 = self.decoder_layers[i](\n",
    "                x, encoding_outputs, training,\n",
    "                look_ahead_mask, padding_mask)\n",
    "            attention_weights[\n",
    "                \"decoder_layer{}_att1\".format(i+1)] = attn1\n",
    "            attention_weights[\n",
    "                \"decoder_layer{}_att2\".format(i+1)] = attn2\n",
    "        # x.shape: (batch_size, output_seq_len, d_model)\n",
    "        return x, attention_weights\n",
    "    \n",
    "sample_decoder_model = DecoderModel(2, 8000, max_length,\n",
    "                                    512, 8, 2048)\n",
    "sample_decoder_model_input = tf.random.uniform((64, 35))\n",
    "sample_decoder_model_output, sample_decoder_model_att \\\n",
    "    = sample_decoder_model(\n",
    "        sample_decoder_model_input,\n",
    "        sample_encoder_model_output,\n",
    "        training=True, look_ahead_mask=None,\n",
    "        padding_mask=None)\n",
    "\n",
    "print(\"sample_decoder_model_output.shape: \", sample_decoder_model_output.shape)\n",
    "for key, value in sample_decoder_model_att.items():\n",
    "    print(\"{}.shape: {}\".format(key, value.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions.shape:  (64, 31, 8000)\n",
      "decoder_layer1_att1.shape: (64, 8, 31, 31)\n",
      "decoder_layer1_att2.shape: (64, 8, 31, 26)\n",
      "decoder_layer2_att1.shape: (64, 8, 31, 31)\n",
      "decoder_layer2_att2.shape: (64, 8, 31, 26)\n"
     ]
    }
   ],
   "source": [
    "class Transformer(keras.models.Model):\n",
    "    def __init__(self, num_layers, input_vocab_size, target_vocab_size,\n",
    "                 max_length, d_model, num_heads, dff, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder_model = EncoderModel(num_layers, input_vocab_size, \n",
    "                                          max_length, d_model, \n",
    "                                          num_heads, dff, rate)\n",
    "        \n",
    "        self.decoder_model = DecoderModel(num_layers, target_vocab_size, \n",
    "                                          max_length, d_model, num_heads,\n",
    "                                          dff, rate)\n",
    "        \n",
    "        self.final_layer = keras.layers.Dense(target_vocab_size)\n",
    "        \n",
    "    def call(self, inp, targ, training, encoder_padding_mask,\n",
    "             look_ahead_mask, decoder_padding_mask):\n",
    "        # encoding_outputs.shape : (batch_size, input_seq_len, d_model)\n",
    "        encoding_outputs = self.encoder_model(inp, training,\n",
    "                                              encoder_padding_mask)\n",
    "        \n",
    "        # decding_outputs.shape : (batch_size, target_seq_len, d_model)\n",
    "        decoding_outputs, attention_weights = self.decoder_model(\n",
    "            targ, encoding_outputs, training, \n",
    "            look_ahead_mask, decoder_padding_mask)\n",
    "        \n",
    "        # predictions.shape : (batch_size, output_seq_len, target_vocab_size)\n",
    "        predictions = self.final_layer(decoding_outputs)\n",
    "        \n",
    "        return predictions, attention_weights\n",
    "sample_transformer = Transformer(2, 8500, 8000, max_length,\n",
    "                                 512, 8, 2048, rate=0.1)\n",
    "temp_input = tf.random.uniform((64, 26))\n",
    "temp_target = tf.random.uniform((64, 31))\n",
    "\n",
    "predictions, attention_weights = sample_transformer(temp_input, \n",
    "                                                    temp_target, \n",
    "                                                    training=False,\n",
    "                                                    encoder_padding_mask=None,\n",
    "                                                    look_ahead_mask=None,\n",
    "                                                    decoder_padding_mask=None)\n",
    "print(\"Predictions.shape: \", predictions.shape)\n",
    "for key, value in attention_weights.items():\n",
    "    print(\"{}.shape: {}\".format(key, value.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 设置超参数， 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. initialize model.\n",
    "# 2. define loss, optimizer, learning_rate schedule\n",
    "# 3.train_step\n",
    "# 4. train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8\n",
    "d_model= 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = pt_tokenizer.vocab_size + 2\n",
    "target_vocab_size = en_tokenizer.vocab_size + 2\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "transformer = Transformer(num_layers,\n",
    "                          input_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          max_length,\n",
    "                          d_model, \n",
    "                          num_heads, \n",
    "                          dff, \n",
    "                          dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 自定义学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate = (d_model ** (-0.5)) * min(step_num ** (-0.5),\n",
    "#                                         step_num * warm_up_steps ** (-1.5))\n",
    "class CustomizedSchedule(\n",
    "    keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomizedSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, dtype=tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** (-1.5))\n",
    "        arg3 = tf.math.rsqrt(self.d_model)\n",
    "        return arg3 * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "learning_rate = CustomizedSchedule(d_model)\n",
    "optimizer = keras.optimizers.Adam(learning_rate,\n",
    "                                  beta_1=0.9,\n",
    "                                  beta_2=0.98,\n",
    "                                  epsilon=1e-9)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train step')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomizedSchedule(d_model)\n",
    "\n",
    "plt.plot(\n",
    "    temp_learning_rate_schedule(\n",
    "        tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.xlabel(\"Train step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction=\"none\")\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.14 Mask 创建与使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    \"\"\"\n",
    "    Encoder:\n",
    "        - encoder_padding_mask (self attention of EncoderLayer)\n",
    "    Decoder:\n",
    "        - look_ahead_mask (self attention of DecoderLayer)\n",
    "        - encoder_decoder_padding_mask (encoder-decoder attnetion of Decoder)\n",
    "        - decoder_padding_mask(self attention of DecoderLayer)\n",
    "    \"\"\"\n",
    "    encoder_padding_mask = create_padding_mask(inp)\n",
    "    encoder_decoder_padding_mask = create_padding_mask(inp)\n",
    "    \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "   \n",
    "    decoder_padding_mask = create_padding_mask(tar)\n",
    "    decoder_mask = tf.maximum(decoder_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_inp.shape:  (64, 36)\n",
      "temp_tar.shape:  (64, 38)\n"
     ]
    }
   ],
   "source": [
    "temp_inp, temp_tar = iter(train_dataset.take(1)).next()\n",
    "\n",
    "print(\"temp_inp.shape: \", temp_inp.shape)\n",
    "print(\"temp_tar.shape: \", temp_tar.shape)\n",
    "encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n",
    "    =create_masks(temp_inp, temp_tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.15 train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:,:-1]\n",
    "    tar_real = tar[:,1:]\n",
    "    \n",
    "    encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n",
    "    = create_masks(inp, tar_inp)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, True,\n",
    "                                     encoder_padding_mask,\n",
    "                                     decoder_mask,\n",
    "                                     encoder_decoder_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(\n",
    "        zip(gradients, transformer.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.8332 Accuracy 11.3606\n",
      "Epoch 1 Batch 1 Loss 4.0749 Accuracy 11.4361\n",
      "Epoch 1 Batch 2 Loss 4.1553 Accuracy 11.5032\n",
      "Epoch 1 Batch 3 Loss 4.1313 Accuracy 11.4858\n",
      "Epoch 1 Batch 4 Loss 4.1955 Accuracy 11.5353\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 5 Loss 4.1893 Accuracy 11.5520\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 6 Loss 4.2202 Accuracy 11.5839\n",
      "WARNING:tensorflow:7 out of the last 8 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 8 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 7 Loss 4.2005 Accuracy 11.5450\n",
      "Epoch 1 Batch 8 Loss 4.2146 Accuracy 11.5328\n",
      "Epoch 1 Batch 9 Loss 4.1846 Accuracy 11.4855\n",
      "Epoch 1 Batch 10 Loss 4.1940 Accuracy 11.4779\n",
      "Epoch 1 Batch 11 Loss 4.1803 Accuracy 11.4433\n",
      "Epoch 1 Batch 12 Loss 4.1713 Accuracy 11.4158\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 13 Loss 4.1971 Accuracy 11.4099\n",
      "Epoch 1 Batch 14 Loss 4.1892 Accuracy 11.3880\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 15 Loss 4.1897 Accuracy 11.3571\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 16 Loss 4.1968 Accuracy 11.3470\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 17 Loss 4.2215 Accuracy 11.3347\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 18 Loss 4.2287 Accuracy 11.3166\n",
      "Epoch 1 Batch 19 Loss 4.2195 Accuracy 11.2898\n",
      "WARNING:tensorflow:7 out of the last 14 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 14 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 20 Loss 4.2259 Accuracy 11.2692\n",
      "Epoch 1 Batch 21 Loss 4.2195 Accuracy 11.2374\n",
      "WARNING:tensorflow:8 out of the last 16 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 16 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 22 Loss 4.2209 Accuracy 11.2166\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 23 Loss 4.2223 Accuracy 11.1993\n",
      "Epoch 1 Batch 24 Loss 4.2300 Accuracy 11.1827\n",
      "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 25 Loss 4.2562 Accuracy 11.1742\n",
      "Epoch 1 Batch 26 Loss 4.2626 Accuracy 11.1558\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 27 Loss 4.2579 Accuracy 11.1300\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 28 Loss 4.2686 Accuracy 11.1113\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 12 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 29 Loss 4.2511 Accuracy 11.0777\n",
      "Epoch 1 Batch 30 Loss 4.2360 Accuracy 11.0342\n",
      "Epoch 1 Batch 31 Loss 4.2288 Accuracy 11.0025\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 11 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 32 Loss 4.2355 Accuracy 10.9860\n",
      "Epoch 1 Batch 33 Loss 4.2372 Accuracy 10.9610\n",
      "Epoch 1 Batch 34 Loss 4.2282 Accuracy 10.9304\n",
      "Epoch 1 Batch 35 Loss 4.2257 Accuracy 10.9010\n",
      "Epoch 1 Batch 36 Loss 4.2239 Accuracy 10.8797\n",
      "Epoch 1 Batch 37 Loss 4.2180 Accuracy 10.8517\n",
      "Epoch 1 Batch 38 Loss 4.2280 Accuracy 10.8384\n",
      "Epoch 1 Batch 39 Loss 4.2305 Accuracy 10.8158\n",
      "Epoch 1 Batch 40 Loss 4.2343 Accuracy 10.7952\n",
      "Epoch 1 Batch 41 Loss 4.2455 Accuracy 10.7826\n",
      "Epoch 1 Batch 42 Loss 4.2436 Accuracy 10.7633\n",
      "Epoch 1 Batch 43 Loss 4.2598 Accuracy 10.7553\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 44 Loss 4.2517 Accuracy 10.7259\n",
      "Epoch 1 Batch 45 Loss 4.2517 Accuracy 10.7068\n",
      "WARNING:tensorflow:6 out of the last 15 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15 calls to <function train_step at 0x00000187467C79D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 46 Loss 4.2457 Accuracy 10.6818\n",
      "Epoch 1 Batch 47 Loss 4.2409 Accuracy 10.6559\n",
      "Epoch 1 Batch 48 Loss 4.2496 Accuracy 10.6433\n",
      "Epoch 1 Batch 49 Loss 4.2418 Accuracy 10.6203\n",
      "Epoch 1 Batch 50 Loss 4.2303 Accuracy 10.5956\n",
      "Epoch 1 Batch 51 Loss 4.2202 Accuracy 10.5703\n",
      "Epoch 1 Batch 52 Loss 4.2133 Accuracy 10.5490\n",
      "Epoch 1 Batch 53 Loss 4.2119 Accuracy 10.5310\n",
      "Epoch 1 Batch 54 Loss 4.2244 Accuracy 10.5258\n",
      "Epoch 1 Batch 55 Loss 4.2288 Accuracy 10.5119\n",
      "Epoch 1 Batch 56 Loss 4.2333 Accuracy 10.4963\n",
      "Epoch 1 Batch 57 Loss 4.2265 Accuracy 10.4748\n",
      "Epoch 1 Batch 58 Loss 4.2270 Accuracy 10.4605\n",
      "Epoch 1 Batch 59 Loss 4.2348 Accuracy 10.4500\n",
      "Epoch 1 Batch 60 Loss 4.2393 Accuracy 10.4377\n",
      "Epoch 1 Batch 61 Loss 4.2329 Accuracy 10.4189\n",
      "Epoch 1 Batch 62 Loss 4.2356 Accuracy 10.4090\n",
      "Epoch 1 Batch 63 Loss 4.2352 Accuracy 10.3982\n",
      "Epoch 1 Batch 64 Loss 4.2377 Accuracy 10.3895\n",
      "Epoch 1 Batch 65 Loss 4.2327 Accuracy 10.3744\n",
      "Epoch 1 Batch 66 Loss 4.2281 Accuracy 10.3622\n",
      "Epoch 1 Batch 67 Loss 4.2290 Accuracy 10.3493\n",
      "Epoch 1 Batch 68 Loss 4.2254 Accuracy 10.3341\n",
      "Epoch 1 Batch 69 Loss 4.2271 Accuracy 10.3238\n",
      "Epoch 1 Batch 70 Loss 4.2302 Accuracy 10.3148\n",
      "Epoch 1 Batch 71 Loss 4.2243 Accuracy 10.2986\n",
      "Epoch 1 Batch 72 Loss 4.2213 Accuracy 10.2860\n",
      "Epoch 1 Batch 73 Loss 4.2186 Accuracy 10.2713\n",
      "Epoch 1 Batch 74 Loss 4.2216 Accuracy 10.2624\n",
      "Epoch 1 Batch 75 Loss 4.2195 Accuracy 10.2497\n",
      "Epoch 1 Batch 76 Loss 4.2142 Accuracy 10.2352\n",
      "Epoch 1 Batch 77 Loss 4.2122 Accuracy 10.2228\n",
      "Epoch 1 Batch 78 Loss 4.2096 Accuracy 10.2100\n",
      "Epoch 1 Batch 79 Loss 4.2019 Accuracy 10.1944\n",
      "Epoch 1 Batch 80 Loss 4.1959 Accuracy 10.1791\n",
      "Epoch 1 Batch 81 Loss 4.1943 Accuracy 10.1667\n",
      "Epoch 1 Batch 82 Loss 4.1928 Accuracy 10.1573\n",
      "Epoch 1 Batch 83 Loss 4.1871 Accuracy 10.1420\n",
      "Epoch 1 Batch 84 Loss 4.1896 Accuracy 10.1344\n",
      "Epoch 1 Batch 85 Loss 4.1894 Accuracy 10.1274\n",
      "Epoch 1 Batch 86 Loss 4.1855 Accuracy 10.1167\n",
      "Epoch 1 Batch 87 Loss 4.1859 Accuracy 10.1074\n",
      "Epoch 1 Batch 88 Loss 4.1829 Accuracy 10.0971\n",
      "Epoch 1 Batch 89 Loss 4.1839 Accuracy 10.0886\n",
      "Epoch 1 Batch 90 Loss 4.1852 Accuracy 10.0810\n",
      "Epoch 1 Batch 91 Loss 4.1806 Accuracy 10.0685\n",
      "Epoch 1 Batch 92 Loss 4.1804 Accuracy 10.0613\n",
      "Epoch 1 Batch 93 Loss 4.1796 Accuracy 10.0537\n",
      "Epoch 1 Batch 94 Loss 4.1770 Accuracy 10.0437\n",
      "Epoch 1 Batch 95 Loss 4.1795 Accuracy 10.0379\n",
      "Epoch 1 Batch 96 Loss 4.1781 Accuracy 10.0286\n",
      "Epoch 1 Batch 97 Loss 4.1813 Accuracy 10.0233\n",
      "Epoch 1 Batch 98 Loss 4.1791 Accuracy 10.0153\n",
      "Epoch 1 Batch 99 Loss 4.1800 Accuracy 10.0083\n",
      "Epoch 1 Batch 100 Loss 4.1840 Accuracy 10.0026\n",
      "Epoch 1 Batch 101 Loss 4.1791 Accuracy 9.9924\n",
      "Epoch 1 Batch 102 Loss 4.1806 Accuracy 9.9845\n",
      "Epoch 1 Batch 103 Loss 4.1809 Accuracy 9.9773\n",
      "Epoch 1 Batch 104 Loss 4.1824 Accuracy 9.9702\n",
      "Epoch 1 Batch 105 Loss 4.1832 Accuracy 9.9636\n",
      "Epoch 1 Batch 106 Loss 4.1792 Accuracy 9.9544\n",
      "Epoch 1 Batch 107 Loss 4.1762 Accuracy 9.9454\n",
      "Epoch 1 Batch 108 Loss 4.1780 Accuracy 9.9393\n",
      "Epoch 1 Batch 109 Loss 4.1763 Accuracy 9.9312\n",
      "Epoch 1 Batch 110 Loss 4.1735 Accuracy 9.9219\n",
      "Epoch 1 Batch 111 Loss 4.1771 Accuracy 9.9165\n",
      "Epoch 1 Batch 112 Loss 4.1749 Accuracy 9.9101\n",
      "Epoch 1 Batch 113 Loss 4.1724 Accuracy 9.9018\n",
      "Epoch 1 Batch 114 Loss 4.1719 Accuracy 9.8964\n",
      "Epoch 1 Batch 115 Loss 4.1703 Accuracy 9.8897\n",
      "Epoch 1 Batch 116 Loss 4.1717 Accuracy 9.8838\n",
      "Epoch 1 Batch 117 Loss 4.1705 Accuracy 9.8777\n",
      "Epoch 1 Batch 118 Loss 4.1729 Accuracy 9.8741\n",
      "Epoch 1 Batch 119 Loss 4.1745 Accuracy 9.8700\n",
      "Epoch 1 Batch 120 Loss 4.1745 Accuracy 9.8661\n",
      "Epoch 1 Batch 121 Loss 4.1749 Accuracy 9.8617\n",
      "Epoch 1 Batch 122 Loss 4.1758 Accuracy 9.8565\n",
      "Epoch 1 Batch 123 Loss 4.1794 Accuracy 9.8537\n",
      "Epoch 1 Batch 124 Loss 4.1758 Accuracy 9.8490\n",
      "Epoch 1 Batch 125 Loss 4.1754 Accuracy 9.8443\n",
      "Epoch 1 Batch 126 Loss 4.1726 Accuracy 9.8419\n",
      "Epoch 1 Batch 127 Loss 4.1724 Accuracy 9.8381\n",
      "Epoch 1 Batch 128 Loss 4.1689 Accuracy 9.8343\n",
      "Epoch 1 Batch 129 Loss 4.1675 Accuracy 9.8329\n",
      "Epoch 1 Batch 130 Loss 4.1632 Accuracy 9.8359\n",
      "Epoch 1 Batch 131 Loss 4.1610 Accuracy 9.8395\n",
      "Epoch 1 Batch 132 Loss 4.1625 Accuracy 9.8478\n",
      "Epoch 1 Batch 133 Loss 4.1652 Accuracy 9.8575\n",
      "Epoch 1 Batch 134 Loss 4.1654 Accuracy 9.8720\n",
      "Epoch 1 Batch 135 Loss 4.1601 Accuracy 9.8924\n",
      "Epoch 1 Batch 136 Loss 4.1594 Accuracy 9.9118\n",
      "Epoch 1 Batch 137 Loss 4.1572 Accuracy 9.9313\n",
      "Epoch 1 Batch 138 Loss 4.1544 Accuracy 9.9555\n",
      "Epoch 1 Batch 139 Loss 4.1539 Accuracy 9.9761\n",
      "Epoch 1 Batch 140 Loss 4.1550 Accuracy 9.9981\n",
      "Epoch 1 Batch 141 Loss 4.1565 Accuracy 10.0172\n",
      "Epoch 1 Batch 142 Loss 4.1542 Accuracy 10.0444\n",
      "Epoch 1 Batch 143 Loss 4.1514 Accuracy 10.0703\n",
      "Epoch 1 Batch 144 Loss 4.1506 Accuracy 10.0948\n",
      "Epoch 1 Batch 145 Loss 4.1457 Accuracy 10.1269\n",
      "Epoch 1 Batch 146 Loss 4.1430 Accuracy 10.1583\n",
      "Epoch 1 Batch 147 Loss 4.1421 Accuracy 10.1882\n",
      "Epoch 1 Batch 148 Loss 4.1393 Accuracy 10.2176\n",
      "Epoch 1 Batch 149 Loss 4.1386 Accuracy 10.2493\n",
      "Epoch 1 Batch 150 Loss 4.1363 Accuracy 10.2823\n",
      "Epoch 1 Batch 151 Loss 4.1328 Accuracy 10.3181\n",
      "Epoch 1 Batch 152 Loss 4.1327 Accuracy 10.3496\n",
      "Epoch 1 Batch 153 Loss 4.1315 Accuracy 10.3801\n",
      "Epoch 1 Batch 154 Loss 4.1286 Accuracy 10.4180\n",
      "Epoch 1 Batch 155 Loss 4.1255 Accuracy 10.4576\n"
     ]
    }
   ],
   "source": [
    "train_loss = keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = keras.metrics.SparseCategoricalCrossentropy(\n",
    "    name=\"train_accuracy\")  # 计算这个值时并没有考虑mask，所以只在一定程度上代表某种趋势  \n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 1 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
    "                epoch+1, batch, train_loss.result(),\n",
    "                train_accuracy.result()))\n",
    "    print(\"Epoch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
    "        epoch+1, train_loss.result(), train_accuracy.result()))\n",
    "    print(\"Time take for 1 epoch: {} sec\\n\".format(\n",
    "        time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 预测与效果展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 evaluate函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    input_id_sentence = [pt_tokenizer.vocab_size] \\\n",
    "        + pt_tokenizer.encode(inp_sentence) + [pt_tokenizer.vocab_size + 1]\n",
    "    # encoder_input.shape : (1, input_sentence_length)\n",
    "    encoder_input = tf.expand_dims(input_id_sentence, 0)\n",
    "    \n",
    "    # decoder_input.shape (1, 1)\n",
    "    decoder_input = tf.expand_dims([en_tokenizer.vocab_size], 0)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        encoder_padding_mask, decoder_mask, encoder_decoder_padding_mask \\\n",
    "        = create_masks(encoder_input, decoder_input)\n",
    "        # predictions.shape: \n",
    "        # (batch_size, output_target_len, target_vocab_size)\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input,\n",
    "            decoder_input,\n",
    "            False,\n",
    "            encoder_padding_mask,\n",
    "            decoder_mask,\n",
    "            encoder_decoder_padding_mask)\n",
    "        # predictions.shape: (batch_size, target_vocab_size)\n",
    "        predictions = predictions[:, -1, :]\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1),\n",
    "                               tf.int32)\n",
    "        \n",
    "        if tf.equal(predicted_id, en_tokenizer.vocab_size + 1):\n",
    "            return tf.squeeze(decoder_input, axis=0), attention_weights\n",
    "        \n",
    "        decoder_input = tf.concat([decoder_input, [predicted_id]], axis=-1)\n",
    "    return tf.squeeze(decoder_input, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 plot函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encoder_decoder_attention(attention, input_sentence,\n",
    "                                   result, layer_name):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    input_id_sentence = pt_tokenizer.encode(input_sentence)\n",
    "    \n",
    "    # attention.shape: (num_heads, tar_len, input_len)\n",
    "    attention = tf.squeeze(attention[layer_name], axis=0)\n",
    "    \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "        \n",
    "        ax.matshow(attention[head][:-1,:])\n",
    "        \n",
    "        fontdict = {\"fontsize\": 10}\n",
    "        \n",
    "        ax.set_xticks(range(len(input_id_sentence) + 2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "        \n",
    "        ax.set_ylim(len(result) - 1.5, -0.5)\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>'] + [pt_tokenizer.decode([i]) for i in input_id_sentence] + ['<end>'],\n",
    "            fontdict=fontdict, rotation=90)\n",
    "        ax.set_yticklabels(\n",
    "            [en_tokenizer.decode([i]) for i in result if i < en_tokenizer.vocab_size],\n",
    "            fontdict=fontdict)\n",
    "        ax.set_xlabel('Head {}'.format(head + 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 translate函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence, layer_name=''):\n",
    "    result, attention_weights = evaluate(input_sentence)\n",
    "    \n",
    "    predicted_sentence = en_tokenizer.decode(\n",
    "        [i for i in result if i < en_tokenizer.vocab_size])\n",
    "    \n",
    "    print(\"Input: {}\".format(input_sentence))\n",
    "    print(\"Predicted translation: {}\".format(predicted_sentence))\n",
    "    \n",
    "    if layer_name:\n",
    "        plot_encoder_decoder_attention(attention_weights, \n",
    "                                       input_sentence,\n",
    "                                       result,\n",
    "                                       layer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('está muito frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('isto é minha vida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('você ainda está em casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('este é o primeiro livro que eu já li',                                       layer_name=\"decoder_layer4_att2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
